{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel connected.\n"
     ]
    }
   ],
   "source": [
    "print(\"Kernel connected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device GPU is too old((6, 1)<7)\n",
      "Using CPU\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, DistilBertTokenizer, DistilBertModel, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import load_dataset, metric\n",
    "import evaluate\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "try:\n",
    "    torch.cuda.empty_cache()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "#torch.set_float32_matmul_precision('high')\n",
    "def check_device(minimum=7):\n",
    "    device=None\n",
    "    cuda=torch.cuda\n",
    "    if cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        capability = torch.cuda.get_device_capability(device)\n",
    "        if capability[0]<minimum:\n",
    "            print(\"Device GPU is too old({}<{})\".format(capability,minimum))\n",
    "            device=None\n",
    "    else:\n",
    "        print(\"Device GPU is unavailable\")\n",
    "    if device is None:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU\")\n",
    "    else:\n",
    "        print(\"Using GPU\")\n",
    "    return device\n",
    "device=check_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceEmbedder():\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        self.model     = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\").to(device)\n",
    "        self.model     = torch.compile(self.model, mode=\"reduce-overhead\")\n",
    "\n",
    "    # Sentence Embedding\n",
    "    # https://www.sbert.net/examples/applications/computing-embeddings/README.html                                                             \n",
    "    def mean_pooling(self, token_embeddings, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        return sum_embeddings / sum_mask\n",
    "\n",
    "    def extract_token_indices(self, input_ids, original_text, start_idx, end_idx):\n",
    "        tokens = self.tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "        start_token_idx, end_token_idx = None, len(tokens)\n",
    "        cursor = 0\n",
    "        for i, token in enumerate(tokens):\n",
    "            if token in [\"[CLS]\", \"[SEP]\", \"[PAD]\"]:\n",
    "                continue\n",
    "\n",
    "            cursor_after_token = cursor + len(token.replace(\"##\", \"\"))\n",
    "            while cursor_after_token < len(original_text) and original_text[cursor_after_token].isspace():\n",
    "                cursor_after_token += 1\n",
    "                \n",
    "            if cursor >= start_idx and start_token_idx is None:\n",
    "                start_token_idx = i\n",
    "            if cursor_after_token >= end_idx:\n",
    "                end_token_idx = i + 1\n",
    "                break\n",
    "            \n",
    "            cursor = cursor_after_token\n",
    "\n",
    "        assert start_token_idx and end_token_idx\n",
    "        return start_token_idx, end_token_idx\n",
    "    \n",
    "    def get_sentence_embeddings(self, sentences):\n",
    "        encoded_input = self.tokenizer(sentences, padding=True, truncation=True, max_length=128, return_tensors='pt').to(device)\n",
    "        with torch.no_grad():\n",
    "            model_output = self.model(**encoded_input)\n",
    "            sentence_embeddings = self.mean_pooling(model_output[0], encoded_input['attention_mask'])\n",
    "            return sentence_embeddings\n",
    "    \n",
    "    def get_sentence_embeddings_focus_on_substring(self, sentences, indices):\n",
    "        encoded_input = self.tokenizer(sentences, padding=True, truncation=True, max_length=128, return_tensors='pt').to(device)\n",
    "        with torch.no_grad():\n",
    "            model_output = self.model(**encoded_input)\n",
    "        \n",
    "        assert model_output[\"last_hidden_state\"].shape[0] == len(sentences)\n",
    "\n",
    "        final_sentence_embeddings = []\n",
    "        for i in range(len(sentences)):\n",
    "            token_idx_start, token_idx_end = self.extract_token_indices(\n",
    "                encoded_input[\"input_ids\"][i], sentences[i], indices[i][0], indices[i][1]\n",
    "            )\n",
    "            token_embeddings = model_output[0][i, token_idx_start:token_idx_end]\n",
    "            attention_mask   = encoded_input['attention_mask'][i, token_idx_start:token_idx_end]\n",
    "            sentence_embeddings = self.mean_pooling(\n",
    "                torch.unsqueeze(token_embeddings, 0), \n",
    "                torch.unsqueeze(attention_mask,   0)\n",
    "            )\n",
    "            final_sentence_embeddings.append(torch.squeeze(sentence_embeddings, 0))\n",
    "\n",
    "        return torch.stack(final_sentence_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_base_dataset():\n",
    "    dataset = load_dataset(\"humicroedit\", \"subtask-1\")\n",
    "\n",
    "    train_df = pd.DataFrame(dataset[\"train\"])\n",
    "    val_df   = pd.DataFrame(dataset[\"validation\"])\n",
    "    test_df  = pd.DataFrame(dataset[\"test\"])\n",
    "\n",
    "    dfs = [train_df, test_df, val_df]\n",
    "    for df in dfs:\n",
    "        def edit_the_headline(original, edit):\n",
    "            openIdx  = original.index(\"<\")\n",
    "            closeIdx = original.index(\"/>\") + len(\"/>\")\n",
    "            return original[:openIdx] + edit + original[closeIdx:]\n",
    "        \n",
    "        df[\"original_sentence\"] = df[\"original\"].apply(lambda s: s.replace(\"<\", \"\").replace(\"/>\", \"\"))\n",
    "        df[\"edited_sentence\"]   = df.apply(lambda row: edit_the_headline(row[\"original\"], row[\"edit\"]), axis=1)\n",
    "\n",
    "        df[\"original_word_start_idx\"] = df[\"original\"].apply(lambda s: s.index(\"<\"))        \n",
    "        df[\"original_word_end_idx\"]   = df[\"original\"].apply(lambda s: s.index(\"/>\") - 1)\n",
    "\n",
    "        df[\"edited_word_start_idx\"] = df[\"original\"].apply(lambda s: s.index(\"<\"))\n",
    "        df[\"edited_word_end_idx\"]   = df.apply(lambda row: row[\"edited_word_start_idx\"] + len(row[\"edit\"]), axis=1)\n",
    "\n",
    "        df[\"all_scores\"]       = df[\"grades\"].apply(lambda s: sorted([int(c) for c in s]))\n",
    "        df[\"normalized_score\"] = df[\"meanGrade\"] / 3.0\n",
    "\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "# Tensor of shape (row_count, 2, embedding_dimensions).\n",
    "# dim=1 signifies the original and the edited sentence (0 original, 1 edited).\n",
    "def precompute_embeddings(df, embedder, compute_embeddings_batch_size=None, device=\"cpu\"):\n",
    "    original_embeddings = []\n",
    "    edited_embeddings   = []\n",
    "    \n",
    "    if compute_embeddings_batch_size is None:\n",
    "        compute_embeddings_batch_size = len(df.index)\n",
    "    print(\"Dataset size:\",len(df.index))\n",
    "    print(\"Batch size:\",compute_embeddings_batch_size)\n",
    "    for i in range(0, len(df.index), compute_embeddings_batch_size):\n",
    "        print(\"{}/{}\".format(i,len(df.index)),end='\\r')\n",
    "        rows = df[i:i + compute_embeddings_batch_size]\n",
    "        original_embeddings.append(embedder.get_sentence_embeddings(rows[\"original_sentence\"].to_list()).to(device))\n",
    "        edited_embeddings  .append(embedder.get_sentence_embeddings(rows[\"edited_sentence\"]  .to_list()).to(device))\n",
    "        \n",
    "    return torch.stack((\n",
    "        torch.cat(original_embeddings), \n",
    "        torch.cat(edited_embeddings)\n",
    "    ), dim=1)\n",
    "\n",
    "def precompute_embeddings_focus_on_edit_word(df, embedder, compute_embeddings_batch_size=None, device=\"cpu\"):\n",
    "    original_embeddings = []\n",
    "    edited_embeddings   = []\n",
    "    \n",
    "    if compute_embeddings_batch_size is None:\n",
    "        compute_embeddings_batch_size = len(df.index)\n",
    "    print(\"Dataset size:\",len(df.index))\n",
    "    print(\"Batch size:\",compute_embeddings_batch_size)\n",
    "    for i in range(0, len(df.index), compute_embeddings_batch_size):\n",
    "        print(\"{}/{}\".format(i,len(df.index)),end='\\r')\n",
    "        rows = df[i:i + compute_embeddings_batch_size]\n",
    "\n",
    "        original = embedder.get_sentence_embeddings_focus_on_substring(\n",
    "            list(rows[\"original_sentence\"]),\n",
    "            list(zip(rows[\"original_word_start_idx\"], rows[\"original_word_end_idx\"]))\n",
    "        )\n",
    "        edited = embedder.get_sentence_embeddings_focus_on_substring(\n",
    "            list(rows[\"edited_sentence\"]),\n",
    "            list(zip(rows[\"edited_word_start_idx\"], rows[\"edited_word_end_idx\"]))\n",
    "        )\n",
    "\n",
    "        original_embeddings.append(original.to(device))\n",
    "        edited_embeddings  .append(edited  .to(device))\n",
    "        \n",
    "    return torch.stack((\n",
    "        torch.cat(original_embeddings), \n",
    "        torch.cat(edited_embeddings)\n",
    "    ), dim=1)\n",
    "\n",
    "class SentenceEmbeddingsDataset(Dataset):\n",
    "    def __init__(self, dataframe, precomputed_embeddings, device=\"cpu\"):\n",
    "        self.df         = dataframe\n",
    "        self.embeddings = precomputed_embeddings\n",
    "        print(f\"{self.embeddings.shape=}\")\n",
    "\n",
    "        score_counts = torch.zeros(len(self.df.index), 4)\n",
    "        for i, scores in self.df[\"all_scores\"].items():\n",
    "            for score in scores:\n",
    "                score_counts[i, score] += 1\n",
    "        self.score_counts = score_counts.to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df.index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx], self.score_counts[idx]\n",
    "    \n",
    "class TokenizedSentencesDataset(Dataset):\n",
    "    def __init__(self, dataframe, device=\"cpu\"):\n",
    "        self.df = dataframe\n",
    "        \n",
    "        score_counts = torch.zeros(len(self.df.index), 4)\n",
    "        for i, scores in self.df[\"all_scores\"].items():\n",
    "            for score in scores:\n",
    "                score_counts[i, score] += 1\n",
    "        self.score_counts = score_counts.to(device)\n",
    "\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', use_fast=True)\n",
    "        output = tokenizer(\n",
    "            self.df[\"original_sentence\"].tolist(),\n",
    "            self.df[\"edited_sentence\"].tolist(), \n",
    "            return_tensors='pt', truncation=True, padding=True\n",
    "        ).to(device)\n",
    "        \n",
    "        self.input_ids      = output[\"input_ids\"]\n",
    "        self.attention_mask = output[\"attention_mask\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df.index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\":      self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_mask[idx],\n",
    "        }, self.score_counts[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset humicroedit (/home/doricirkveni/.cache/huggingface/datasets/humicroedit/subtask-1/0.0.0/209c209bc70671d8d939aefd82e51f8ff294e47504ec64ef653a93a1f13e9ed3)\n",
      "100%|██████████| 4/4 [00:00<00:00, 64.56it/s]\n"
     ]
    }
   ],
   "source": [
    "cached_sentence_embedder = SentenceEmbedder()\n",
    "cached_base_dataset = make_base_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 frames\n",
      "Dataset size: 9652\n",
      "Batch size: 512\n",
      "0/9652\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/doricirkveni/miniconda3/envs/tpenv/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:279: UserWarning: changing options to `torch.compile()` may require calling `torch._dynamo.reset()` to take effect\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.embeddings.shape=torch.Size([9652, 2, 384])\n",
      "Dataset size: 2419\n",
      "Batch size: 512\n",
      "self.embeddings.shape=torch.Size([2419, 2, 384])\n",
      "Dataset size: 3024\n",
      "Batch size: 512\n",
      "self.embeddings.shape=torch.Size([3024, 2, 384])\n"
     ]
    }
   ],
   "source": [
    "print(len(cached_base_dataset),\"frames\")\n",
    "cached_dataset_naive_sentence_embeddings = [\n",
    "    SentenceEmbeddingsDataset(\n",
    "        df, \n",
    "        precompute_embeddings(df, cached_sentence_embedder, compute_embeddings_batch_size=512, device=device),\n",
    "        device=device\n",
    "    )\n",
    "    for df in cached_base_dataset\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 9652\n",
      "Batch size: 512\n",
      "self.embeddings.shape=torch.Size([9652, 2, 384])\n",
      "Dataset size: 2419\n",
      "Batch size: 512\n",
      "self.embeddings.shape=torch.Size([2419, 2, 384])\n",
      "Dataset size: 3024\n",
      "Batch size: 512\n",
      "self.embeddings.shape=torch.Size([3024, 2, 384])\n"
     ]
    }
   ],
   "source": [
    "cached_dataset_edited_words_embeddings = [\n",
    "    SentenceEmbeddingsDataset(\n",
    "        df, \n",
    "        precompute_embeddings_focus_on_edit_word(df, cached_sentence_embedder, compute_embeddings_batch_size=512, device=device),\n",
    "        device=device\n",
    "    )\n",
    "    for df in cached_base_dataset\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_dataset_tokenized_sentences = [TokenizedSentencesDataset(df, device=device) for df in cached_base_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OptimizedModule(\n",
      "  (_orig_mod): Sequential(\n",
      "    (0): Dropout(p=0.3, inplace=False)\n",
      "    (1): Linear(in_features=768, out_features=10, bias=True)\n",
      "    (2): Tanh()\n",
      "    (3): Linear(in_features=10, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-06-12 18:14:29,730] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-06-12 18:14:30,109] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.9226214408874511, Val Loss: 0.8758575320243835\n",
      "Epoch 1, RMSE: 0.9605318531352571, Val RMSE: 0.9358726045912358\n",
      "Epoch 11, Loss: 0.5103295981884003, Val Loss: 0.8758575320243835\n",
      "Epoch 11, RMSE: 0.7143735704716407, Val RMSE: 0.9358726045912358\n",
      "Epoch 21, Loss: 0.388301020860672, Val Loss: 0.8758575320243835\n",
      "Epoch 21, RMSE: 0.6231380431819838, Val RMSE: 0.9358726045912358\n",
      "Epoch 31, Loss: 0.3580849885940552, Val Loss: 0.8758575320243835\n",
      "Epoch 31, RMSE: 0.5984020292362445, Val RMSE: 0.9358726045912358\n",
      "Epoch 41, Loss: 0.3460699081420898, Val Loss: 0.8758575320243835\n",
      "Epoch 41, RMSE: 0.588277067496337, Val RMSE: 0.9358726045912358\n",
      "Epoch 51, Loss: 0.33938300609588623, Val Loss: 0.8758575320243835\n",
      "Epoch 51, RMSE: 0.5825658813352239, Val RMSE: 0.9358726045912358\n",
      "Epoch 61, Loss: 0.33566907048225403, Val Loss: 0.8758575320243835\n",
      "Epoch 61, RMSE: 0.5793695456979544, Val RMSE: 0.9358726045912358\n",
      "Epoch 71, Loss: 0.33232282400131224, Val Loss: 0.8758575320243835\n",
      "Epoch 71, RMSE: 0.5764744781872935, Val RMSE: 0.9358726045912358\n",
      "Epoch 81, Loss: 0.33142414689064026, Val Loss: 0.8758575320243835\n",
      "Epoch 81, RMSE: 0.5756944909330297, Val RMSE: 0.9358726045912358\n",
      "Epoch 91, Loss: 0.32631808519363403, Val Loss: 0.8758575320243835\n",
      "Epoch 91, RMSE: 0.5712425799900022, Val RMSE: 0.9358726045912358\n",
      "Epoch 101, Loss: 0.3230582356452942, Val Loss: 0.8758575320243835\n",
      "Epoch 101, RMSE: 0.5683821211520417, Val RMSE: 0.9358726045912358\n",
      "Epoch 111, Loss: 0.3229555130004883, Val Loss: 0.8758575320243835\n",
      "Epoch 111, RMSE: 0.5682917498965547, Val RMSE: 0.9358726045912358\n",
      "Epoch 121, Loss: 0.318877512216568, Val Loss: 0.8758575320243835\n",
      "Epoch 121, RMSE: 0.5646924049573963, Val RMSE: 0.9358726045912358\n",
      "Epoch 131, Loss: 0.3186845719814301, Val Loss: 0.8758575320243835\n",
      "Epoch 131, RMSE: 0.5645215425308675, Val RMSE: 0.9358726045912358\n",
      "Epoch 141, Loss: 0.317256098985672, Val Loss: 0.8758575320243835\n",
      "Epoch 141, RMSE: 0.5632549147461317, Val RMSE: 0.9358726045912358\n",
      "Epoch 151, Loss: 0.31503169536590575, Val Loss: 0.8758575320243835\n",
      "Epoch 151, RMSE: 0.5612768437820197, Val RMSE: 0.9358726045912358\n",
      "Epoch 161, Loss: 0.31345882415771487, Val Loss: 0.8758575320243835\n",
      "Epoch 161, RMSE: 0.5598739359514022, Val RMSE: 0.9358726045912358\n",
      "Epoch 171, Loss: 0.31236907839775085, Val Loss: 0.8758575320243835\n",
      "Epoch 171, RMSE: 0.5588998822667176, Val RMSE: 0.9358726045912358\n",
      "Epoch 181, Loss: 0.3107675969600677, Val Loss: 0.8758575320243835\n",
      "Epoch 181, RMSE: 0.557465332518595, Val RMSE: 0.9358726045912358\n",
      "Epoch 191, Loss: 0.3102599740028381, Val Loss: 0.8758575320243835\n",
      "Epoch 191, RMSE: 0.5570098509028706, Val RMSE: 0.9358726045912358\n",
      "Test Loss: 0.5934740602970123\n",
      "Test RMSE: 0.7703726762398913\n"
     ]
    }
   ],
   "source": [
    "import openpyxl\n",
    "def experiment_predict_mean_score(train_dataset, val_dataset, test_dataset):\n",
    "    batch_size   = 2048\n",
    "    num_epochs   = 200\n",
    "    lr           = 1e-4\n",
    "    weight_decay = 0\n",
    "\n",
    "\n",
    "    score_weights = torch.arange(0, 4, dtype=torch.float, device=device).unsqueeze(1)\n",
    "    def reshape_batch(batch):\n",
    "        inputs, labels = batch\n",
    "        inputs = torch.flatten(inputs, start_dim=-2)\n",
    "        labels = labels / torch.sum(labels, dim=-1, keepdim=True)\n",
    "        actual_mean_score = torch.squeeze(labels @ score_weights) / 3\n",
    "        return inputs, actual_mean_score\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(val_dataset,   batch_size=batch_size)\n",
    "    test_loader  = DataLoader(test_dataset,  batch_size=batch_size)\n",
    "\n",
    "    one_embedding, _ = train_dataset[0]\n",
    "    input_features = torch.numel(one_embedding)\n",
    "\n",
    "    layers = []\n",
    "    #while input_features > 50:\n",
    "    #    layers.append(nn.Linear(input_features, input_features // 2))\n",
    "    #    layers.append(nn.Dropout1d(0.1))\n",
    "    #    layers.append(nn.Tanh())\n",
    "    #    input_features //= 2\n",
    "    \n",
    "    layers.append(nn.Dropout(0.3))\n",
    "    layers.append(nn.Linear(input_features, 10))\n",
    "    layers.append(nn.Tanh())\n",
    "    layers.append(nn.Linear(10, 1))\n",
    "    #layers.append(nn.Sigmoid())\n",
    "\n",
    "    model = nn.Sequential(*layers)\n",
    "    model = model.to(device)\n",
    "    model = torch.compile(model)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    print(model)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = reshape_batch(data)\n",
    "            labels *= 3\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(val_loader, 0):\n",
    "                inputs, labels = reshape_batch(data)\n",
    "                labels *= 3\n",
    "                outputs = model(inputs).squeeze()\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}, Val Loss: {val_loss/len(val_loader)}\")\n",
    "            print(f\"Epoch {epoch+1}, RMSE: {np.sqrt(running_loss/len(train_loader))}, Val RMSE: {np.sqrt(val_loss/len(val_loader))}\")\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader, 0):\n",
    "            inputs, labels = reshape_batch(data)\n",
    "            labels *= 3\n",
    "            outputs = model(inputs).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "    print(f\"Test Loss: {test_loss/len(test_loader)}\")\n",
    "    print(f\"Test RMSE: {np.sqrt(test_loss/len(test_loader))}\")\n",
    "\n",
    "    # Excel output\n",
    "    predicted_scores = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader, 0):\n",
    "            inputs, labels = reshape_batch(data)\n",
    "            labels *= 3\n",
    "            outputs = model(inputs).squeeze()\n",
    "            predicted_scores.extend(outputs.tolist())\n",
    "            \n",
    "    excel_df = test_dataset.df.copy()\n",
    "    excel_df[\"normalized_predicted_score\"] = predicted_scores\n",
    "    excel_df.to_excel(\"experiment_predict_mean_score.xlsx\")\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch._dynamo.reset()\n",
    "\n",
    "experiment_predict_mean_score(*cached_dataset_naive_sentence_embeddings)\n",
    "#experiment_predict_mean_score(*cached_dataset_edited_words_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-06-12 18:15:15,653] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OptimizedModule(\n",
      "  (_orig_mod): Sequential(\n",
      "    (0): Dropout(p=0.3, inplace=False)\n",
      "    (1): Linear(in_features=768, out_features=4, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-06-12 18:15:18,891] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.4011665105819702, Val Loss: 1.388827383518219\n",
      "Epoch 11, Loss: 1.3221417903900146, Val Loss: 1.388827383518219\n",
      "Epoch 21, Loss: 1.2893345832824707, Val Loss: 1.388827383518219\n",
      "Epoch 31, Loss: 1.2738260746002197, Val Loss: 1.388827383518219\n",
      "Epoch 41, Loss: 1.2665892839431763, Val Loss: 1.388827383518219\n",
      "Epoch 51, Loss: 1.260304594039917, Val Loss: 1.388827383518219\n",
      "Epoch 61, Loss: 1.2568406581878662, Val Loss: 1.388827383518219\n",
      "Epoch 71, Loss: 1.2546792507171631, Val Loss: 1.388827383518219\n",
      "Epoch 81, Loss: 1.251495099067688, Val Loss: 1.388827383518219\n",
      "Epoch 91, Loss: 1.249082612991333, Val Loss: 1.388827383518219\n",
      "Epoch 101, Loss: 1.2472482204437256, Val Loss: 1.388827383518219\n",
      "Epoch 111, Loss: 1.246453547477722, Val Loss: 1.388827383518219\n",
      "Epoch 121, Loss: 1.2449327945709228, Val Loss: 1.388827383518219\n",
      "Epoch 131, Loss: 1.2430774927139283, Val Loss: 1.388827383518219\n",
      "Epoch 141, Loss: 1.2424615621566772, Val Loss: 1.388827383518219\n",
      "Epoch 151, Loss: 1.2418817281723022, Val Loss: 1.388827383518219\n",
      "Epoch 161, Loss: 1.2405004739761352, Val Loss: 1.388827383518219\n",
      "Epoch 171, Loss: 1.23861243724823, Val Loss: 1.388827383518219\n",
      "Epoch 181, Loss: 1.2394316434860229, Val Loss: 1.388827383518219\n",
      "Epoch 191, Loss: 1.2376527070999146, Val Loss: 1.388827383518219\n",
      "Epoch 201, Loss: 1.2375762939453125, Val Loss: 1.388827383518219\n",
      "Epoch 211, Loss: 1.2356896638870238, Val Loss: 1.388827383518219\n",
      "Epoch 221, Loss: 1.2363852500915526, Val Loss: 1.388827383518219\n",
      "Epoch 231, Loss: 1.2358781576156617, Val Loss: 1.388827383518219\n",
      "Epoch 241, Loss: 1.2354920387268067, Val Loss: 1.388827383518219\n",
      "Epoch 251, Loss: 1.2335240364074707, Val Loss: 1.388827383518219\n",
      "Epoch 261, Loss: 1.2336308240890503, Val Loss: 1.388827383518219\n",
      "Epoch 271, Loss: 1.2340250253677367, Val Loss: 1.388827383518219\n",
      "Epoch 281, Loss: 1.2340880155563354, Val Loss: 1.388827383518219\n",
      "Epoch 291, Loss: 1.233411931991577, Val Loss: 1.388827383518219\n",
      "Epoch 301, Loss: 1.2328285455703736, Val Loss: 1.388827383518219\n",
      "Epoch 311, Loss: 1.2322146892547607, Val Loss: 1.388827383518219\n",
      "Epoch 321, Loss: 1.232296347618103, Val Loss: 1.388827383518219\n",
      "Epoch 331, Loss: 1.2310834169387816, Val Loss: 1.388827383518219\n",
      "Epoch 341, Loss: 1.2313061475753784, Val Loss: 1.388827383518219\n",
      "Epoch 351, Loss: 1.2314208984375, Val Loss: 1.388827383518219\n",
      "Epoch 361, Loss: 1.2305759906768798, Val Loss: 1.388827383518219\n",
      "Epoch 371, Loss: 1.2301383018493652, Val Loss: 1.388827383518219\n",
      "Epoch 381, Loss: 1.2304495573043823, Val Loss: 1.388827383518219\n",
      "Epoch 391, Loss: 1.2301371574401856, Val Loss: 1.388827383518219\n",
      "Epoch 401, Loss: 1.2315358161926269, Val Loss: 1.388827383518219\n",
      "Epoch 411, Loss: 1.2286213874816894, Val Loss: 1.388827383518219\n",
      "Epoch 421, Loss: 1.2286837339401244, Val Loss: 1.388827383518219\n",
      "Epoch 431, Loss: 1.229486346244812, Val Loss: 1.388827383518219\n",
      "Epoch 441, Loss: 1.229265069961548, Val Loss: 1.388827383518219\n",
      "Epoch 451, Loss: 1.2292478799819946, Val Loss: 1.388827383518219\n",
      "Epoch 461, Loss: 1.2292335033416748, Val Loss: 1.388827383518219\n",
      "Epoch 471, Loss: 1.2283830881118774, Val Loss: 1.388827383518219\n",
      "Epoch 481, Loss: 1.2277880668640138, Val Loss: 1.388827383518219\n",
      "Epoch 491, Loss: 1.2281203269958496, Val Loss: 1.388827383518219\n",
      "Test Loss: 1.3185615539550781\n"
     ]
    }
   ],
   "source": [
    "def experiment_predict_score_distribution(train_dataset, val_dataset, test_dataset):\n",
    "    batch_size   = 2048\n",
    "    num_epochs   = 500\n",
    "    lr           = 1e-4\n",
    "    weight_decay = 0\n",
    "\n",
    "    def reshape_batch(batch):\n",
    "        inputs, labels = batch\n",
    "        inputs = torch.flatten(inputs, start_dim=-2)\n",
    "        labels = labels / torch.sum(labels, dim=-1, keepdim=True)\n",
    "        return inputs, labels\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(val_dataset,   batch_size=batch_size)\n",
    "    test_loader  = DataLoader(test_dataset,  batch_size=batch_size)\n",
    "\n",
    "    one_embedding, _ = train_dataset[0]\n",
    "    input_features = torch.numel(one_embedding)\n",
    "\n",
    "    layers = []\n",
    "    #while input_features > 50:\n",
    "    #    layers.append(nn.Linear(input_features, input_features // 2))\n",
    "    #    layers.append(nn.Dropout1d(0.1))\n",
    "    #    layers.append(nn.Tanh())\n",
    "    #    input_features //= 2\n",
    "    \n",
    "    layers.append(nn.Dropout(0.3))\n",
    "    layers.append(nn.Linear(input_features, 4))\n",
    "    #layers.append(nn.Tanh())\n",
    "    #layers.append(nn.Linear(10, 4)) # logits\n",
    "\n",
    "    model = nn.Sequential(*layers)\n",
    "    model = torch.compile(model.to(device))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    print(model)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = reshape_batch(data)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for i, data in enumerate(val_loader, 0):\n",
    "                    inputs, labels = reshape_batch(data)\n",
    "                    outputs = model(inputs).squeeze()\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}, Val Loss: {val_loss/len(val_loader)}\")\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader, 0):\n",
    "            inputs, labels = reshape_batch(data)\n",
    "            outputs = model(inputs).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "    print(f\"Test Loss: {test_loss/len(test_loader)}\")\n",
    "\n",
    "    # Excel output\n",
    "    predicted_scores = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader, 0):\n",
    "            inputs, labels = reshape_batch(data)\n",
    "            logits = model(inputs).squeeze()\n",
    "\n",
    "            p = torch.softmax(logits, dim=1)\n",
    "            score_weights = torch.arange(0, 4, dtype=torch.float, device=device).unsqueeze(1)\n",
    "            mean_score = torch.squeeze(p @ score_weights) / 3\n",
    "            predicted_scores.extend(mean_score.tolist())\n",
    "            \n",
    "    excel_df = test_dataset.df.copy()\n",
    "    excel_df[\"normalized_predicted_score\"] = predicted_scores\n",
    "    excel_df.to_excel(\"experiment_predict_score_distribution.xlsx\")\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch._dynamo.reset()\n",
    "\n",
    "#experiment_predict_score_distribution(*cached_dataset_edited_words_embeddings)\n",
    "experiment_predict_score_distribution(*cached_dataset_naive_sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Read timed out.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mtimeout\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/site-packages/urllib3/response.py:710\u001b[0m, in \u001b[0;36mHTTPResponse._error_catcher\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 710\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[1;32m    712\u001b[0m \u001b[39mexcept\u001b[39;00m SocketTimeout \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    713\u001b[0m     \u001b[39m# FIXME: Ideally we'd like to include the url in the ReadTimeoutError but\u001b[39;00m\n\u001b[1;32m    714\u001b[0m     \u001b[39m# there is yet no clean way to get at it from this context.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/site-packages/urllib3/response.py:814\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 814\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp_read(amt) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fp_closed \u001b[39melse\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    815\u001b[0m     \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m data:\n\u001b[1;32m    816\u001b[0m         \u001b[39m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    817\u001b[0m         \u001b[39m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    822\u001b[0m         \u001b[39m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m         \u001b[39m# no harm in redundantly calling close.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/site-packages/urllib3/response.py:799\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    798\u001b[0m     \u001b[39m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 799\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49mread(amt) \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mread()\n",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/http/client.py:463\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    462\u001b[0m b \u001b[39m=\u001b[39m \u001b[39mbytearray\u001b[39m(amt)\n\u001b[0;32m--> 463\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[1;32m    464\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mmemoryview\u001b[39m(b)[:n]\u001b[39m.\u001b[39mtobytes()\n",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/http/client.py:507\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[39m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \u001b[39m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[1;32m    506\u001b[0m \u001b[39m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[0;32m--> 507\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[1;32m    508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m n \u001b[39mand\u001b[39;00m b:\n\u001b[1;32m    509\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    510\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    705\u001b[0m \u001b[39mexcept\u001b[39;00m timeout:\n",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/ssl.py:1242\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1239\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1240\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1241\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1242\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1243\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/ssl.py:1100\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1099\u001b[0m \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1100\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1101\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mtimeout\u001b[0m: The read operation timed out",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    815\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/site-packages/urllib3/response.py:940\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    939\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp) \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 940\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(amt\u001b[39m=\u001b[39;49mamt, decode_content\u001b[39m=\u001b[39;49mdecode_content)\n\u001b[1;32m    942\u001b[0m     \u001b[39mif\u001b[39;00m data:\n",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/site-packages/urllib3/response.py:879\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    877\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer\u001b[39m.\u001b[39mget(amt)\n\u001b[0;32m--> 879\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raw_read(amt)\n\u001b[1;32m    881\u001b[0m flush_decoder \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/site-packages/urllib3/response.py:835\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    825\u001b[0m         \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    826\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menforce_content_length\n\u001b[1;32m    827\u001b[0m             \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength_remaining \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    833\u001b[0m             \u001b[39m# raised during streaming, so all calls with incorrect\u001b[39;00m\n\u001b[1;32m    834\u001b[0m             \u001b[39m# Content-Length are caught.\u001b[39;00m\n\u001b[0;32m--> 835\u001b[0m             \u001b[39mraise\u001b[39;00m IncompleteRead(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp_bytes_read, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength_remaining)\n\u001b[1;32m    837\u001b[0m \u001b[39mif\u001b[39;00m data:\n",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen\u001b[39m.\u001b[39;49mthrow(typ, value, traceback)\n\u001b[1;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m    139\u001b[0m     \u001b[39m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     \u001b[39m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     \u001b[39m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/site-packages/urllib3/response.py:715\u001b[0m, in \u001b[0;36mHTTPResponse._error_catcher\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[39mexcept\u001b[39;00m SocketTimeout \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    713\u001b[0m     \u001b[39m# FIXME: Ideally we'd like to include the url in the ReadTimeoutError but\u001b[39;00m\n\u001b[1;32m    714\u001b[0m     \u001b[39m# there is yet no clean way to get at it from this context.\u001b[39;00m\n\u001b[0;32m--> 715\u001b[0m     \u001b[39mraise\u001b[39;00m ReadTimeoutError(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pool, \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mRead timed out.\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    717\u001b[0m \u001b[39mexcept\u001b[39;00m BaseSSLError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    718\u001b[0m     \u001b[39m# FIXME: Is there a better way to differentiate between SSLErrors?\u001b[39;00m\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m: HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Read timed out.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 114\u001b[0m\n\u001b[1;32m    111\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n\u001b[1;32m    112\u001b[0m torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mreset()\n\u001b[0;32m--> 114\u001b[0m experiment_predict_score_distribution_finetune_distillbert(\u001b[39m*\u001b[39;49mcached_dataset_tokenized_sentences)\n",
      "Cell \u001b[0;32mIn[25], line 17\u001b[0m, in \u001b[0;36mexperiment_predict_score_distribution_finetune_distillbert\u001b[0;34m(train_dataset, val_dataset, test_dataset)\u001b[0m\n\u001b[1;32m     14\u001b[0m val_loader   \u001b[39m=\u001b[39m DataLoader(val_dataset,   batch_size\u001b[39m=\u001b[39mbatch_size)\n\u001b[1;32m     15\u001b[0m test_loader  \u001b[39m=\u001b[39m DataLoader(test_dataset,  batch_size\u001b[39m=\u001b[39mbatch_size)\n\u001b[0;32m---> 17\u001b[0m distilbert \u001b[39m=\u001b[39m DistilBertModel\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m'\u001b[39;49m\u001b[39mdistilbert-base-uncased\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     18\u001b[0m distilbert \u001b[39m=\u001b[39m distilbert\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     19\u001b[0m \u001b[39m#distilbert = torch.compile(distilbert)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/site-packages/transformers/modeling_utils.py:2494\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2479\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   2480\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m   2481\u001b[0m     cached_file_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m   2482\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcache_dir\u001b[39m\u001b[39m\"\u001b[39m: cache_dir,\n\u001b[1;32m   2483\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mforce_download\u001b[39m\u001b[39m\"\u001b[39m: force_download,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2492\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m: commit_hash,\n\u001b[1;32m   2493\u001b[0m     }\n\u001b[0;32m-> 2494\u001b[0m     resolved_archive_file \u001b[39m=\u001b[39m cached_file(pretrained_model_name_or_path, filename, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcached_file_kwargs)\n\u001b[1;32m   2496\u001b[0m     \u001b[39m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[1;32m   2497\u001b[0m     \u001b[39m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[1;32m   2498\u001b[0m     \u001b[39mif\u001b[39;00m resolved_archive_file \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m filename \u001b[39m==\u001b[39m _add_variant(SAFE_WEIGHTS_NAME, variant):\n\u001b[1;32m   2499\u001b[0m         \u001b[39m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/site-packages/transformers/utils/hub.py:417\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    414\u001b[0m user_agent \u001b[39m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    415\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    416\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 417\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    418\u001b[0m         path_or_repo_id,\n\u001b[1;32m    419\u001b[0m         filename,\n\u001b[1;32m    420\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[1;32m    421\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[1;32m    422\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    423\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    424\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    425\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    426\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    427\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    428\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    429\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    430\u001b[0m     )\n\u001b[1;32m    432\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[1;32m    433\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    434\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m is not a local folder and is not a valid model identifier \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    435\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlisted on \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIf this is a private repository, make sure to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    436\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpass a token having permission to this repo with `use_auth_token` or log in with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    437\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`huggingface-cli login` and pass `use_auth_token=True`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    438\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/site-packages/huggingface_hub/file_download.py:1364\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1361\u001b[0m \u001b[39mwith\u001b[39;00m temp_file_manager() \u001b[39mas\u001b[39;00m temp_file:\n\u001b[1;32m   1362\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mdownloading \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m to \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, url, temp_file\u001b[39m.\u001b[39mname)\n\u001b[0;32m-> 1364\u001b[0m     http_get(\n\u001b[1;32m   1365\u001b[0m         url_to_download,\n\u001b[1;32m   1366\u001b[0m         temp_file,\n\u001b[1;32m   1367\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1368\u001b[0m         resume_size\u001b[39m=\u001b[39;49mresume_size,\n\u001b[1;32m   1369\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m   1370\u001b[0m         expected_size\u001b[39m=\u001b[39;49mexpected_size,\n\u001b[1;32m   1371\u001b[0m     )\n\u001b[1;32m   1373\u001b[0m \u001b[39mif\u001b[39;00m local_dir \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mStoring \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m in cache at \u001b[39m\u001b[39m{\u001b[39;00mblob_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/site-packages/huggingface_hub/file_download.py:541\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, timeout, max_retries, expected_size)\u001b[0m\n\u001b[1;32m    531\u001b[0m     displayed_name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(…)\u001b[39m\u001b[39m{\u001b[39;00mdisplayed_name[\u001b[39m-\u001b[39m\u001b[39m20\u001b[39m:]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    533\u001b[0m progress \u001b[39m=\u001b[39m tqdm(\n\u001b[1;32m    534\u001b[0m     unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mB\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    535\u001b[0m     unit_scale\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    539\u001b[0m     disable\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m(logger\u001b[39m.\u001b[39mgetEffectiveLevel() \u001b[39m==\u001b[39m logging\u001b[39m.\u001b[39mNOTSET),\n\u001b[1;32m    540\u001b[0m )\n\u001b[0;32m--> 541\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m r\u001b[39m.\u001b[39miter_content(chunk_size\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m \u001b[39m*\u001b[39m \u001b[39m1024\u001b[39m \u001b[39m*\u001b[39m \u001b[39m1024\u001b[39m):\n\u001b[1;32m    542\u001b[0m     \u001b[39mif\u001b[39;00m chunk:  \u001b[39m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[1;32m    543\u001b[0m         progress\u001b[39m.\u001b[39mupdate(\u001b[39mlen\u001b[39m(chunk))\n",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/site-packages/requests/models.py:822\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[39mraise\u001b[39;00m ContentDecodingError(e)\n\u001b[1;32m    821\u001b[0m \u001b[39mexcept\u001b[39;00m ReadTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 822\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(e)\n\u001b[1;32m    823\u001b[0m \u001b[39mexcept\u001b[39;00m SSLError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    824\u001b[0m     \u001b[39mraise\u001b[39;00m RequestsSSLError(e)\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Read timed out."
     ]
    }
   ],
   "source": [
    "def experiment_predict_score_distribution_finetune_distillbert(train_dataset, val_dataset, test_dataset):\n",
    "    batch_size   = 16 \n",
    "    num_epochs   = 1000\n",
    "    lr           = 1e-4\n",
    "    lr_bert      = 3e-5\n",
    "    weight_decay = 0\n",
    "\n",
    "    def reshape_batch(batch):\n",
    "        inputs, labels = batch\n",
    "        labels = labels / torch.sum(labels, dim=-1, keepdim=True)\n",
    "        return inputs, labels\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(val_dataset,   batch_size=batch_size)\n",
    "    test_loader  = DataLoader(test_dataset,  batch_size=batch_size)\n",
    "\n",
    "    distilbert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "    distilbert = distilbert.to(device)\n",
    "    #distilbert = torch.compile(distilbert)\n",
    "\n",
    "    for param in distilbert.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    layers = [\n",
    "              nn.Linear(768, 768),\n",
    "              nn.Dropout1d(0.3),\n",
    "              nn.Tanh(),\n",
    "              nn.Linear(768, 4)] # logits\n",
    "\n",
    "    model = nn.Sequential(*layers)\n",
    "    model = torch.compile(model.to(device))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam([\n",
    "        { \"params\": distilbert.parameters(), \"lr\": lr_bert },\n",
    "        { \"params\": model.parameters(),      \"lr\": lr, \"weight_decay\": weight_decay },\n",
    "    ])\n",
    "\n",
    "    print(model)\n",
    "\n",
    "    def logits_from_inputs(distilbert, model, inputs):\n",
    "        outputs = distilbert(**inputs).last_hidden_state[:, 0, :]\n",
    "        logits = model(outputs).squeeze()\n",
    "        return logits\n",
    "        \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = reshape_batch(data)\n",
    "            logits = logits_from_inputs(distilbert, model, inputs)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for i, data in enumerate(val_loader, 0):\n",
    "                    inputs, labels = reshape_batch(data)\n",
    "                    logits = logits_from_inputs(distilbert, model, inputs)\n",
    "                    loss = criterion(logits, labels)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}, Val Loss: {val_loss/len(val_loader)}\")\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader, 0):\n",
    "            inputs, labels = reshape_batch(data)\n",
    "            logits = logits_from_inputs(distilbert, model, inputs)\n",
    "            loss = criterion(logits, labels)\n",
    "            test_loss += loss.item()\n",
    "    print(f\"Test Loss: {test_loss/len(test_loader)}\")\n",
    "\n",
    "    # evaluate the model and print the RMS loss on the test set\n",
    "    model.eval()\n",
    "    mse_loss = torch.nn.MSELoss()\n",
    "    with torch.no_grad():\n",
    "        predictions = []\n",
    "        actuals = []\n",
    "        for i, data in enumerate(test_loader, 0):\n",
    "            inputs, labels = reshape_batch(data)\n",
    "            logits = logits_from_inputs(distilbert, model, inputs)\n",
    "            predictions.extend(torch.argmax(logits, dim=1).tolist())\n",
    "            actuals.extend(torch.argmax(labels, dim=1).tolist())\n",
    "        print(f\"Test Accuracy: {metric.compute(predictions=predictions, references=actuals)}\")\n",
    "\n",
    "    # Excel output\n",
    "    predicted_scores = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader, 0):\n",
    "            inputs, labels = reshape_batch(data)\n",
    "            logits = logits_from_inputs(distilbert, model, inputs)\n",
    "\n",
    "            p = torch.softmax(logits, dim=1)\n",
    "            score_weights = torch.arange(0, 4, dtype=torch.float, device=device).unsqueeze(1)\n",
    "            mean_score = torch.squeeze(p @ score_weights) / 3\n",
    "            predicted_scores.extend(mean_score.tolist())\n",
    "            \n",
    "    excel_df = test_dataset.df.copy()\n",
    "    excel_df[\"normalized_predicted_score\"] = predicted_scores\n",
    "    excel_df.to_excel(\"experiment_predict_score_distribution_finetune_distillbert.xlsx\")\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch._dynamo.reset()\n",
    "\n",
    "experiment_predict_score_distribution_finetune_distillbert(*cached_dataset_tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 1.73MB/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Read timed out.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mtimeout\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/site-packages/urllib3/response.py:710\u001b[0m, in \u001b[0;36mHTTPResponse._error_catcher\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 710\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[1;32m    712\u001b[0m \u001b[39mexcept\u001b[39;00m SocketTimeout \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    713\u001b[0m     \u001b[39m# FIXME: Ideally we'd like to include the url in the ReadTimeoutError but\u001b[39;00m\n\u001b[1;32m    714\u001b[0m     \u001b[39m# there is yet no clean way to get at it from this context.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/site-packages/urllib3/response.py:814\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 814\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp_read(amt) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fp_closed \u001b[39melse\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    815\u001b[0m     \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m data:\n\u001b[1;32m    816\u001b[0m         \u001b[39m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    817\u001b[0m         \u001b[39m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    822\u001b[0m         \u001b[39m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m         \u001b[39m# no harm in redundantly calling close.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/site-packages/urllib3/response.py:799\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    798\u001b[0m     \u001b[39m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 799\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49mread(amt) \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mread()\n",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/http/client.py:463\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    462\u001b[0m b \u001b[39m=\u001b[39m \u001b[39mbytearray\u001b[39m(amt)\n\u001b[0;32m--> 463\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[1;32m    464\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mmemoryview\u001b[39m(b)[:n]\u001b[39m.\u001b[39mtobytes()\n",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/http/client.py:507\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[39m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \u001b[39m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[1;32m    506\u001b[0m \u001b[39m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[0;32m--> 507\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[1;32m    508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m n \u001b[39mand\u001b[39;00m b:\n\u001b[1;32m    509\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    510\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    705\u001b[0m \u001b[39mexcept\u001b[39;00m timeout:\n",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/ssl.py:1242\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1239\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1240\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1241\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1242\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1243\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/ssl.py:1100\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1099\u001b[0m \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1100\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1101\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mtimeout\u001b[0m: The read operation timed out",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    815\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/site-packages/urllib3/response.py:940\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    939\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp) \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 940\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(amt\u001b[39m=\u001b[39;49mamt, decode_content\u001b[39m=\u001b[39;49mdecode_content)\n\u001b[1;32m    942\u001b[0m     \u001b[39mif\u001b[39;00m data:\n",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/site-packages/urllib3/response.py:879\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    877\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer\u001b[39m.\u001b[39mget(amt)\n\u001b[0;32m--> 879\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raw_read(amt)\n\u001b[1;32m    881\u001b[0m flush_decoder \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/site-packages/urllib3/response.py:835\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    825\u001b[0m         \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    826\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menforce_content_length\n\u001b[1;32m    827\u001b[0m             \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength_remaining \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    833\u001b[0m             \u001b[39m# raised during streaming, so all calls with incorrect\u001b[39;00m\n\u001b[1;32m    834\u001b[0m             \u001b[39m# Content-Length are caught.\u001b[39;00m\n\u001b[0;32m--> 835\u001b[0m             \u001b[39mraise\u001b[39;00m IncompleteRead(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp_bytes_read, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength_remaining)\n\u001b[1;32m    837\u001b[0m \u001b[39mif\u001b[39;00m data:\n",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen\u001b[39m.\u001b[39;49mthrow(typ, value, traceback)\n\u001b[1;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m    139\u001b[0m     \u001b[39m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     \u001b[39m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     \u001b[39m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/site-packages/urllib3/response.py:715\u001b[0m, in \u001b[0;36mHTTPResponse._error_catcher\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[39mexcept\u001b[39;00m SocketTimeout \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    713\u001b[0m     \u001b[39m# FIXME: Ideally we'd like to include the url in the ReadTimeoutError but\u001b[39;00m\n\u001b[1;32m    714\u001b[0m     \u001b[39m# there is yet no clean way to get at it from this context.\u001b[39;00m\n\u001b[0;32m--> 715\u001b[0m     \u001b[39mraise\u001b[39;00m ReadTimeoutError(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pool, \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mRead timed out.\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    717\u001b[0m \u001b[39mexcept\u001b[39;00m BaseSSLError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    718\u001b[0m     \u001b[39m# FIXME: Is there a better way to differentiate between SSLErrors?\u001b[39;00m\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m: HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Read timed out.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 88\u001b[0m\n\u001b[1;32m     85\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n\u001b[1;32m     86\u001b[0m torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mreset()\n\u001b[0;32m---> 88\u001b[0m experiment_predict_mean_score_finetune_distilbert_huggingface(\u001b[39m*\u001b[39;49mcached_base_dataset)\n",
      "Cell \u001b[0;32mIn[26], line 24\u001b[0m, in \u001b[0;36mexperiment_predict_mean_score_finetune_distilbert_huggingface\u001b[0;34m(train_df, val_df, test_df)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[39mreturn\u001b[39;00m { \u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m: actual_mean_score, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs }\n\u001b[1;32m     23\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mdistilbert-base-uncased\u001b[39m\u001b[39m'\u001b[39m, use_fast\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 24\u001b[0m model \u001b[39m=\u001b[39m AutoModelForSequenceClassification\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m'\u001b[39;49m\u001b[39mdistilbert-base-uncased\u001b[39;49m\u001b[39m'\u001b[39;49m, num_labels\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     25\u001b[0m \u001b[39m#model = torch.compile(model.to(device))\u001b[39;00m\n\u001b[1;32m     27\u001b[0m args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[1;32m     28\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mdistilbert-base-uncased-finetuned-subtask-1\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     29\u001b[0m     evaluation_strategy \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msteps\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m     metric_for_best_model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmse\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     40\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:484\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    483\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 484\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    485\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    486\u001b[0m     )\n\u001b[1;32m    487\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    488\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    489\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/site-packages/transformers/modeling_utils.py:2494\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2479\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   2480\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m   2481\u001b[0m     cached_file_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m   2482\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcache_dir\u001b[39m\u001b[39m\"\u001b[39m: cache_dir,\n\u001b[1;32m   2483\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mforce_download\u001b[39m\u001b[39m\"\u001b[39m: force_download,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2492\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m: commit_hash,\n\u001b[1;32m   2493\u001b[0m     }\n\u001b[0;32m-> 2494\u001b[0m     resolved_archive_file \u001b[39m=\u001b[39m cached_file(pretrained_model_name_or_path, filename, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcached_file_kwargs)\n\u001b[1;32m   2496\u001b[0m     \u001b[39m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[1;32m   2497\u001b[0m     \u001b[39m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[1;32m   2498\u001b[0m     \u001b[39mif\u001b[39;00m resolved_archive_file \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m filename \u001b[39m==\u001b[39m _add_variant(SAFE_WEIGHTS_NAME, variant):\n\u001b[1;32m   2499\u001b[0m         \u001b[39m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/site-packages/transformers/utils/hub.py:417\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    414\u001b[0m user_agent \u001b[39m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    415\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    416\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 417\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    418\u001b[0m         path_or_repo_id,\n\u001b[1;32m    419\u001b[0m         filename,\n\u001b[1;32m    420\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[1;32m    421\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[1;32m    422\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    423\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    424\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    425\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    426\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    427\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    428\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    429\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    430\u001b[0m     )\n\u001b[1;32m    432\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[1;32m    433\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    434\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m is not a local folder and is not a valid model identifier \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    435\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlisted on \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIf this is a private repository, make sure to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    436\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpass a token having permission to this repo with `use_auth_token` or log in with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    437\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`huggingface-cli login` and pass `use_auth_token=True`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    438\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/site-packages/huggingface_hub/file_download.py:1364\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1361\u001b[0m \u001b[39mwith\u001b[39;00m temp_file_manager() \u001b[39mas\u001b[39;00m temp_file:\n\u001b[1;32m   1362\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mdownloading \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m to \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, url, temp_file\u001b[39m.\u001b[39mname)\n\u001b[0;32m-> 1364\u001b[0m     http_get(\n\u001b[1;32m   1365\u001b[0m         url_to_download,\n\u001b[1;32m   1366\u001b[0m         temp_file,\n\u001b[1;32m   1367\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1368\u001b[0m         resume_size\u001b[39m=\u001b[39;49mresume_size,\n\u001b[1;32m   1369\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m   1370\u001b[0m         expected_size\u001b[39m=\u001b[39;49mexpected_size,\n\u001b[1;32m   1371\u001b[0m     )\n\u001b[1;32m   1373\u001b[0m \u001b[39mif\u001b[39;00m local_dir \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mStoring \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m in cache at \u001b[39m\u001b[39m{\u001b[39;00mblob_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/site-packages/huggingface_hub/file_download.py:541\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, timeout, max_retries, expected_size)\u001b[0m\n\u001b[1;32m    531\u001b[0m     displayed_name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(…)\u001b[39m\u001b[39m{\u001b[39;00mdisplayed_name[\u001b[39m-\u001b[39m\u001b[39m20\u001b[39m:]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    533\u001b[0m progress \u001b[39m=\u001b[39m tqdm(\n\u001b[1;32m    534\u001b[0m     unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mB\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    535\u001b[0m     unit_scale\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    539\u001b[0m     disable\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m(logger\u001b[39m.\u001b[39mgetEffectiveLevel() \u001b[39m==\u001b[39m logging\u001b[39m.\u001b[39mNOTSET),\n\u001b[1;32m    540\u001b[0m )\n\u001b[0;32m--> 541\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m r\u001b[39m.\u001b[39miter_content(chunk_size\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m \u001b[39m*\u001b[39m \u001b[39m1024\u001b[39m \u001b[39m*\u001b[39m \u001b[39m1024\u001b[39m):\n\u001b[1;32m    542\u001b[0m     \u001b[39mif\u001b[39;00m chunk:  \u001b[39m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[1;32m    543\u001b[0m         progress\u001b[39m.\u001b[39mupdate(\u001b[39mlen\u001b[39m(chunk))\n",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/site-packages/requests/models.py:822\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[39mraise\u001b[39;00m ContentDecodingError(e)\n\u001b[1;32m    821\u001b[0m \u001b[39mexcept\u001b[39;00m ReadTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 822\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(e)\n\u001b[1;32m    823\u001b[0m \u001b[39mexcept\u001b[39;00m SSLError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    824\u001b[0m     \u001b[39mraise\u001b[39;00m RequestsSSLError(e)\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Read timed out."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    }
   ],
   "source": [
    "def experiment_predict_mean_score_finetune_distilbert_huggingface(train_df, val_df, test_df):\n",
    "    train_dataset = TokenizedSentencesDataset(train_df, device=\"cpu\")\n",
    "    val_dataset   = TokenizedSentencesDataset(val_df,   device=\"cpu\")\n",
    "    test_dataset  = TokenizedSentencesDataset(test_df,  device=\"cpu\")\n",
    "\n",
    "    batch_size   = 16\n",
    "    num_epochs   = 4\n",
    "    lr           = 5e-5\n",
    "    weight_decay = 0.0001\n",
    "\n",
    "    class NormalizedLabelsDataset(Dataset):\n",
    "        def __init__(self, base_dataset):\n",
    "            self.base_dataset = base_dataset\n",
    "        def __len__(self):\n",
    "            return len(self.base_dataset)\n",
    "        def __getitem__(self, idx):\n",
    "            inputs, labels = self.base_dataset[idx]\n",
    "            labels = labels / torch.sum(labels, dim=-1, keepdim=True)\n",
    "            score_weights     = torch.arange(0, 4, dtype=torch.float).unsqueeze(1)\n",
    "            actual_mean_score = torch.squeeze(labels @ score_weights) / 3\n",
    "            return { \"labels\": actual_mean_score, **inputs }\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased', use_fast=True)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=1)\n",
    "    #model = torch.compile(model.to(device))\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        \"distilbert-base-uncased-finetuned-subtask-1\",\n",
    "        evaluation_strategy = \"steps\",\n",
    "        eval_steps=200,\n",
    "        save_strategy = \"steps\",\n",
    "        save_steps=200,\n",
    "        learning_rate=lr,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=num_epochs,\n",
    "        weight_decay=weight_decay,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"mse\",\n",
    "    )\n",
    "\n",
    "    mae_metric = evaluate.load(\"mae\")\n",
    "    mse_metric = evaluate.load(\"mse\")\n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        predictions, labels = torch.tensor(predictions),  torch.tensor(labels)\n",
    "        predictions, labels = torch.squeeze(predictions), torch.squeeze(labels)\n",
    "        predictions *= 3\n",
    "        labels      *= 3\n",
    "        \n",
    "        #p = torch.softmax(predictions, dim=1)\n",
    "        #score_weights = torch.arange(0, 4, dtype=torch.float).unsqueeze(1)\n",
    "        #predicted_mean_score = torch.squeeze(p      @ score_weights) / 3\n",
    "        #actual_mean_score    = torch.squeeze(labels @ score_weights) / 3\n",
    "        #return metric.compute(predictions=predicted_mean_score, references=actual_mean_score)\n",
    "        mse = mse_metric.compute(predictions=predictions, references=labels)\n",
    "        mae = mae_metric.compute(predictions=predictions, references=labels)\n",
    "        return {\n",
    "            \"mae\":  mae[\"mae\"],\n",
    "            \"mse\":  mse[\"mse\"],\n",
    "            \"rmse\": np.sqrt(mse[\"mse\"]),\n",
    "        }\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        args,\n",
    "        train_dataset=NormalizedLabelsDataset(train_dataset),\n",
    "        eval_dataset=NormalizedLabelsDataset(val_dataset),\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.evaluate()\n",
    "    out = trainer.predict(NormalizedLabelsDataset(test_dataset))\n",
    "    predictions = np.squeeze(out.predictions)\n",
    "    # compute metrics\n",
    "    predictions *= 3\n",
    "    \n",
    "\n",
    "    excel_df = test_dataset.df.copy()\n",
    "    excel_df[\"normalized_predicted_score\"] = predictions\n",
    "    excel_df.to_excel(\"experiment_predict_mean_score_finetune_distillbert.xlsx\")\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch._dynamo.reset()\n",
    "\n",
    "experiment_predict_mean_score_finetune_distilbert_huggingface(*cached_base_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading model.safetensors:  31%|███▏      | 83.9M/268M [12:22<27:09, 113kB/s] \n",
      "Downloading model.safetensors:  63%|██████▎   | 168M/268M [09:09<05:28, 305kB/s] \n",
      "Downloading model.safetensors:  27%|██▋       | 73.4M/268M [06:33<17:22, 187kB/s] \n",
      "Downloading (…)e9125/.gitattributes: 100%|██████████| 1.18k/1.18k [00:00<00:00, 10.8kB/s]\n",
      "Downloading (…)_Pooling/config.json: 100%|██████████| 190/190 [00:00<00:00, 55.7kB/s]\n",
      "Downloading (…)7e55de9125/README.md: 100%|██████████| 10.6k/10.6k [00:00<00:00, 6.70MB/s]\n",
      "Downloading (…)55de9125/config.json: 100%|██████████| 612/612 [00:00<00:00, 151kB/s]\n",
      "Downloading (…)ce_transformers.json: 100%|██████████| 116/116 [00:00<00:00, 42.8kB/s]\n",
      "Downloading (…)125/data_config.json: 100%|██████████| 39.3k/39.3k [00:00<00:00, 1.36MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 90.9M/90.9M [00:27<00:00, 3.28MB/s]\n",
      "Downloading (…)nce_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 31.9kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 41.8kB/s]\n",
      "Downloading (…)e9125/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 2.05MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 350/350 [00:00<00:00, 59.0kB/s]\n",
      "Downloading (…)9125/train_script.py: 100%|██████████| 13.2k/13.2k [00:00<00:00, 3.78MB/s]\n",
      "Downloading (…)7e55de9125/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 1.92MB/s]\n",
      "Downloading (…)5de9125/modules.json: 100%|██████████| 349/349 [00:00<00:00, 92.5kB/s]\n",
      "Iteration: 100%|██████████| 1/1 [00:05<00:00,  5.73s/it]\n",
      "Epoch: 100%|██████████| 1/1 [00:05<00:00,  5.74s/it]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import openai\n",
    "openai.api_key = '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'\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, LoggingHandler, losses, InputExample\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def make_base_dataset():\n",
    "    dataset = load_dataset(\"humicroedit\", \"subtask-1\")\n",
    "\n",
    "    train_df = pd.DataFrame(dataset[\"train\"])\n",
    "    val_df   = pd.DataFrame(dataset[\"validation\"])\n",
    "    test_df  = pd.DataFrame(dataset[\"test\"])\n",
    "\n",
    "    dfs = [train_df, test_df, val_df]\n",
    "    for df in dfs:\n",
    "        def edit_the_headline(original, edit):\n",
    "            openIdx  = original.index(\"<\")\n",
    "            closeIdx = original.index(\"/>\") + len(\"/>\")\n",
    "            return original[:openIdx] + edit + original[closeIdx:]\n",
    "        \n",
    "        df[\"original_sentence\"] = df[\"original\"].apply(lambda s: s.replace(\"<\", \"\").replace(\"/>\", \"\"))\n",
    "        df[\"edited_sentence\"]   = df.apply(lambda row: edit_the_headline(row[\"original\"], row[\"edit\"]), axis=1)\n",
    "\n",
    "        df[\"original_word_start_idx\"] = df[\"original\"].apply(lambda s: s.index(\"<\"))        \n",
    "        df[\"original_word_end_idx\"]   = df[\"original\"].apply(lambda s: s.index(\"/>\") - 1)\n",
    "\n",
    "        df[\"edited_word_start_idx\"] = df[\"original\"].apply(lambda s: s.index(\"<\"))\n",
    "        df[\"edited_word_end_idx\"]   = df.apply(lambda row: row[\"edited_word_start_idx\"] + len(row[\"edit\"]), axis=1)\n",
    "\n",
    "        df[\"all_scores\"]       = df[\"grades\"].apply(lambda s: sorted([int(c) for c in s]))\n",
    "        df[\"normalized_score\"] = df[\"meanGrade\"] / 3.0\n",
    "\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "train_examples = [\n",
    "    InputExample(texts=['This is a positive pair', 'Where the distance will be minimized'], label=1),\n",
    "    InputExample(texts=['This is a negative pair', 'Their distance will be increased'], label=0)]\n",
    "\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=2)\n",
    "train_loss = losses.ContrastiveLoss(model=model)\n",
    "\n",
    "model.fit([(train_dataloader, train_loss)], show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import openai\n",
    "openai.api_key = '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'\n",
    "\n",
    "train_df = cached_base_dataset[0]\n",
    "# random sample of 10 from train_df\n",
    "\n",
    "top_df = train_df.sort_values(by=[\"meanGrade\"], ascending=True).head(50).sample(5)\n",
    "bot_df = train_df.sort_values(by=[\"meanGrade\"], ascending=False).head(50).sample(5)\n",
    "\n",
    "a = list(train_df[\"original\"].sample(10)) + list(top_df[\"original\"]) + list(bot_df[\"original\"])\n",
    "b = list(train_df[\"edit\"].sample(10))     + list(top_df[\"edit\"])     + list(bot_df[\"edit\"])\n",
    "for aa, bb in zip(a, b):\n",
    "    aa = aa.replace(\"<\", \"[ \").replace(\"/>\", f\" => {bb} ]\")\n",
    "    print(aa)\n",
    "\n",
    "def generate_explanations(df, filename, completions_per_headline=1):\n",
    "    prompt = \"\"\"\n",
    "        The following news headlines have been edited to be more humorous.\n",
    "        The format of the headline is \"text text [[ original word => edited word ]] text text\".\n",
    "        Explain what kind of humorous response the edit wanted to elicit, and wether it suceeeded or fell flat.\n",
    "        You are not to be too easily offended. Answer as concisely as possible. When explaining something refer to the exact part in the headline.\n",
    "        Do not use more than 3 sentences. Only output the explanation, nothing else.\n",
    "\n",
    "        Headline:\n",
    "        REPLACE_WITH_HEADLINE\n",
    "    \"\"\"\n",
    "\n",
    "    completions = defaultdict(list)\n",
    "    for i in range(len(df.index)):\n",
    "        headline = df.iloc[i]\n",
    "        original = headline[\"original\"]\n",
    "        edit     = headline[\"edit\"]\n",
    "        combined = original.replace(\"<\", \"[[ \").replace(\"/>\", f\" => {edit} ]]\")\n",
    "        \n",
    "        this_prompt = prompt.replace(\"REPLACE_WITH_HEADLINE\", combined)\n",
    "\n",
    "        print(\"generating\", i, \"/\", len(df.index))\n",
    "\n",
    "        response = None\n",
    "        while True:\n",
    "            try:\n",
    "                response = openai.ChatCompletion.create(\n",
    "                    model=\"gpt-3.5-turbo\",\n",
    "                    messages=[\n",
    "                        { \"role\": \"user\", \"content\": this_prompt }\n",
    "                    ],\n",
    "                    temperature=0.7,\n",
    "                    max_tokens=128,\n",
    "                    n=completions_per_headline\n",
    "                )\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(\"retrying...\")\n",
    "                time.sleep(1)\n",
    "\n",
    "        for choice in response[\"choices\"]:\n",
    "            completions[original].append(choice[\"message\"][\"content\"])\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(completions, f, indent=4)\n",
    "\n",
    "    return completions\n",
    "\n",
    "def generate_explanations2(df, filename, completions_per_headline=1):\n",
    "    prompt = \"\"\"\n",
    "        The following news headlines have been edited to be more humorous.\n",
    "        The format of the headline is \"text text [[ original word => edited word ]] text text\".\n",
    "        Explain what kind of humorous response the edit wanted to elicit, and wether it suceeeded or fell flat.\n",
    "        You are not to be too easily offended. Answer as concisely as possible. When explaining something refer to the exact part in the headline.\n",
    "        Do not use more than 3 sentences. Only output the explanation, nothing else.\n",
    "\n",
    "        Headline:\n",
    "        REPLACE_WITH_HEADLINE\n",
    "    \"\"\"\n",
    "\n",
    "    completions = defaultdict(list)\n",
    "    for i in range(len(df.index)):\n",
    "        headline = df.iloc[i]\n",
    "        original = headline[\"original\"]\n",
    "        edit     = headline[\"edit\"]\n",
    "        combined = original.replace(\"<\", \"[[ \").replace(\"/>\", f\" => {edit} ]]\")\n",
    "        \n",
    "        this_prompt = prompt.replace(\"REPLACE_WITH_HEADLINE\", combined)\n",
    "\n",
    "        print(\"generating\", i, \"/\", len(df.index))\n",
    "\n",
    "        request = {\n",
    "            'user_input': this_prompt,\n",
    "            'history': {'internal': [], 'visible': []},\n",
    "            'mode': 'instruct',  # Valid options: 'chat', 'chat-instruct', 'instruct'\n",
    "            'character': 'Example',\n",
    "            'instruction_template': 'Vicuna-v1.1',\n",
    "\n",
    "            'regenerate': False,\n",
    "            '_continue': False,\n",
    "            'stop_at_newline': True,\n",
    "            'chat_prompt_size': 2048,\n",
    "            'chat_generation_attempts': 1,\n",
    "            'chat-instruct_command': 'Continue the chat dialogue below. Write a single reply for the character \"<|character|>\".\\n\\n<|prompt|>',\n",
    "\n",
    "            'max_new_tokens': 250,\n",
    "            'do_sample': True,\n",
    "            'temperature': 0.7,\n",
    "            'top_p': 0.1,\n",
    "            'typical_p': 1,\n",
    "            'repetition_penalty': 1.18,\n",
    "            'top_k': 40,\n",
    "            'min_length': 0,\n",
    "            'no_repeat_ngram_size': 0,\n",
    "            'num_beams': 1,\n",
    "            'penalty_alpha': 0,\n",
    "            'length_penalty': 1,\n",
    "            'early_stopping': True,\n",
    "            'seed': -1,\n",
    "            'add_bos_token': True,\n",
    "            'truncation_length': 2048,\n",
    "            'ban_eos_token': False,\n",
    "            'skip_special_tokens': True,\n",
    "            'stopping_strings': []\n",
    "        }\n",
    "\n",
    "        response = requests.post(\"http://127.0.0.1:5000/api/v1/chat\", json=request)\n",
    "        print(response.status_code)\n",
    "        if response.status_code == 200:\n",
    "            result = response.json()['results'][0]['history']\n",
    "            print(json.dumps(result, indent=4))\n",
    "            print()\n",
    "            print(result['visible'][-1][1])\n",
    "            completions[original].append(result['visible'][-1][1])\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(completions, f, indent=4)\n",
    "\n",
    "    return completions\n",
    "\n",
    "\n",
    "x = generate_explanations2(cached_base_dataset[0], \"explanations_train.json\", 1)\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"completions_1703.json\", \"w\") as f:\n",
    "    json.dump(completions, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Environment",
   "language": "python",
   "name": "tpenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
