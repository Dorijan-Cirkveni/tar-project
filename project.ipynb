{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel connected.\n"
     ]
    }
   ],
   "source": [
    "print(\"Kernel connected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/doricirkveni/miniconda3/envs/tpenv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device GPU is too old((6, 1)<7)\n",
      "Using CPU\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, DistilBertTokenizer, DistilBertModel, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import load_dataset, metric\n",
    "import evaluate\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "try:\n",
    "    torch.cuda.empty_cache()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "#torch.set_float32_matmul_precision('high')\n",
    "def check_device(minimum=7):\n",
    "    device=None\n",
    "    cuda=torch.cuda\n",
    "    if cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        capability = torch.cuda.get_device_capability(device)\n",
    "        if capability[0]<minimum:\n",
    "            print(\"Device GPU is too old({}<{})\".format(capability,minimum))\n",
    "            device=None\n",
    "    else:\n",
    "        print(\"Device GPU is unavailable\")\n",
    "    if device is None:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU\")\n",
    "    else:\n",
    "        print(\"Using GPU\")\n",
    "    return device\n",
    "device=check_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceEmbedder():\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        self.model     = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\").to(device)\n",
    "        self.model     = torch.compile(self.model, mode=\"reduce-overhead\")\n",
    "\n",
    "    # Sentence Embedding\n",
    "    # https://www.sbert.net/examples/applications/computing-embeddings/README.html                                                             \n",
    "    def mean_pooling(self, token_embeddings, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        return sum_embeddings / sum_mask\n",
    "\n",
    "    def extract_token_indices(self, input_ids, original_text, start_idx, end_idx):\n",
    "        tokens = self.tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "        start_token_idx, end_token_idx = None, len(tokens)\n",
    "        cursor = 0\n",
    "        for i, token in enumerate(tokens):\n",
    "            if token in [\"[CLS]\", \"[SEP]\", \"[PAD]\"]:\n",
    "                continue\n",
    "\n",
    "            cursor_after_token = cursor + len(token.replace(\"##\", \"\"))\n",
    "            while cursor_after_token < len(original_text) and original_text[cursor_after_token].isspace():\n",
    "                cursor_after_token += 1\n",
    "                \n",
    "            if cursor >= start_idx and start_token_idx is None:\n",
    "                start_token_idx = i\n",
    "            if cursor_after_token >= end_idx:\n",
    "                end_token_idx = i + 1\n",
    "                break\n",
    "            \n",
    "            cursor = cursor_after_token\n",
    "\n",
    "        assert start_token_idx and end_token_idx\n",
    "        return start_token_idx, end_token_idx\n",
    "    \n",
    "    def get_sentence_embeddings(self, sentences):\n",
    "        encoded_input = self.tokenizer(sentences, padding=True, truncation=True, max_length=128, return_tensors='pt').to(device)\n",
    "        with torch.no_grad():\n",
    "            model_output = self.model(**encoded_input)\n",
    "            sentence_embeddings = self.mean_pooling(model_output[0], encoded_input['attention_mask'])\n",
    "            return sentence_embeddings\n",
    "    \n",
    "    def get_sentence_embeddings_focus_on_substring(self, sentences, indices):\n",
    "        encoded_input = self.tokenizer(sentences, padding=True, truncation=True, max_length=128, return_tensors='pt').to(device)\n",
    "        with torch.no_grad():\n",
    "            model_output = self.model(**encoded_input)\n",
    "        \n",
    "        assert model_output[\"last_hidden_state\"].shape[0] == len(sentences)\n",
    "\n",
    "        final_sentence_embeddings = []\n",
    "        for i in range(len(sentences)):\n",
    "            token_idx_start, token_idx_end = self.extract_token_indices(\n",
    "                encoded_input[\"input_ids\"][i], sentences[i], indices[i][0], indices[i][1]\n",
    "            )\n",
    "            token_embeddings = model_output[0][i, token_idx_start:token_idx_end]\n",
    "            attention_mask   = encoded_input['attention_mask'][i, token_idx_start:token_idx_end]\n",
    "            sentence_embeddings = self.mean_pooling(\n",
    "                torch.unsqueeze(token_embeddings, 0), \n",
    "                torch.unsqueeze(attention_mask,   0)\n",
    "            )\n",
    "            final_sentence_embeddings.append(torch.squeeze(sentence_embeddings, 0))\n",
    "\n",
    "        return torch.stack(final_sentence_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_base_dataset():\n",
    "    dataset = load_dataset(\"humicroedit\", \"subtask-1\")\n",
    "\n",
    "    train_df = pd.DataFrame(dataset[\"train\"])\n",
    "    val_df   = pd.DataFrame(dataset[\"validation\"])\n",
    "    test_df  = pd.DataFrame(dataset[\"test\"])\n",
    "\n",
    "    dfs = [train_df, test_df, val_df]\n",
    "    for df in dfs:\n",
    "        def edit_the_headline(original, edit):\n",
    "            openIdx  = original.index(\"<\")\n",
    "            closeIdx = original.index(\"/>\") + len(\"/>\")\n",
    "            return original[:openIdx] + edit + original[closeIdx:]\n",
    "        \n",
    "        df[\"original_sentence\"] = df[\"original\"].apply(lambda s: s.replace(\"<\", \"\").replace(\"/>\", \"\"))\n",
    "        df[\"edited_sentence\"]   = df.apply(lambda row: edit_the_headline(row[\"original\"], row[\"edit\"]), axis=1)\n",
    "\n",
    "        df[\"original_word_start_idx\"] = df[\"original\"].apply(lambda s: s.index(\"<\"))        \n",
    "        df[\"original_word_end_idx\"]   = df[\"original\"].apply(lambda s: s.index(\"/>\") - 1)\n",
    "\n",
    "        df[\"edited_word_start_idx\"] = df[\"original\"].apply(lambda s: s.index(\"<\"))\n",
    "        df[\"edited_word_end_idx\"]   = df.apply(lambda row: row[\"edited_word_start_idx\"] + len(row[\"edit\"]), axis=1)\n",
    "\n",
    "        df[\"all_scores\"]       = df[\"grades\"].apply(lambda s: sorted([int(c) for c in s]))\n",
    "        df[\"normalized_score\"] = df[\"meanGrade\"] / 3.0\n",
    "\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "# Tensor of shape (row_count, 2, embedding_dimensions).\n",
    "# dim=1 signifies the original and the edited sentence (0 original, 1 edited).\n",
    "def precompute_embeddings(df, embedder, compute_embeddings_batch_size=None, device=\"cpu\"):\n",
    "    original_embeddings = []\n",
    "    edited_embeddings   = []\n",
    "    \n",
    "    if compute_embeddings_batch_size is None:\n",
    "        compute_embeddings_batch_size = len(df.index)\n",
    "    print(\"Dataset size:\",len(df.index))\n",
    "    print(\"Batch size:\",compute_embeddings_batch_size)\n",
    "    for i in range(0, len(df.index), compute_embeddings_batch_size):\n",
    "        print(\"{}/{}\".format(i,len(df.index)),end='\\r')\n",
    "        rows = df[i:i + compute_embeddings_batch_size]\n",
    "        original_embeddings.append(embedder.get_sentence_embeddings(rows[\"original_sentence\"].to_list()).to(device))\n",
    "        edited_embeddings  .append(embedder.get_sentence_embeddings(rows[\"edited_sentence\"]  .to_list()).to(device))\n",
    "        \n",
    "    return torch.stack((\n",
    "        torch.cat(original_embeddings), \n",
    "        torch.cat(edited_embeddings)\n",
    "    ), dim=1)\n",
    "\n",
    "def precompute_embeddings_focus_on_edit_word(df, embedder, compute_embeddings_batch_size=None, device=\"cpu\"):\n",
    "    original_embeddings = []\n",
    "    edited_embeddings   = []\n",
    "    \n",
    "    if compute_embeddings_batch_size is None:\n",
    "        compute_embeddings_batch_size = len(df.index)\n",
    "    print(\"Dataset size:\",len(df.index))\n",
    "    print(\"Batch size:\",compute_embeddings_batch_size)\n",
    "    for i in range(0, len(df.index), compute_embeddings_batch_size):\n",
    "        print(\"{}/{}\".format(i,len(df.index)),end='\\r')\n",
    "        rows = df[i:i + compute_embeddings_batch_size]\n",
    "\n",
    "        original = embedder.get_sentence_embeddings_focus_on_substring(\n",
    "            list(rows[\"original_sentence\"]),\n",
    "            list(zip(rows[\"original_word_start_idx\"], rows[\"original_word_end_idx\"]))\n",
    "        )\n",
    "        edited = embedder.get_sentence_embeddings_focus_on_substring(\n",
    "            list(rows[\"edited_sentence\"]),\n",
    "            list(zip(rows[\"edited_word_start_idx\"], rows[\"edited_word_end_idx\"]))\n",
    "        )\n",
    "\n",
    "        original_embeddings.append(original.to(device))\n",
    "        edited_embeddings  .append(edited  .to(device))\n",
    "        \n",
    "    return torch.stack((\n",
    "        torch.cat(original_embeddings), \n",
    "        torch.cat(edited_embeddings)\n",
    "    ), dim=1)\n",
    "\n",
    "class SentenceEmbeddingsDataset(Dataset):\n",
    "    def __init__(self, dataframe, precomputed_embeddings, device=\"cpu\"):\n",
    "        self.df         = dataframe\n",
    "        self.embeddings = precomputed_embeddings\n",
    "        print(f\"{self.embeddings.shape=}\")\n",
    "\n",
    "        score_counts = torch.zeros(len(self.df.index), 4)\n",
    "        for i, scores in self.df[\"all_scores\"].items():\n",
    "            for score in scores:\n",
    "                score_counts[i, score] += 1\n",
    "        self.score_counts = score_counts.to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df.index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx], self.score_counts[idx]\n",
    "    \n",
    "class TokenizedSentencesDataset(Dataset):\n",
    "    def __init__(self, dataframe, device=\"cpu\"):\n",
    "        self.df = dataframe\n",
    "        \n",
    "        score_counts = torch.zeros(len(self.df.index), 4)\n",
    "        for i, scores in self.df[\"all_scores\"].items():\n",
    "            for score in scores:\n",
    "                score_counts[i, score] += 1\n",
    "        self.score_counts = score_counts.to(device)\n",
    "\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', use_fast=True)\n",
    "        output = tokenizer(\n",
    "            self.df[\"original_sentence\"].tolist(),\n",
    "            self.df[\"edited_sentence\"].tolist(), \n",
    "            return_tensors='pt', truncation=True, padding=True\n",
    "        ).to(device)\n",
    "        \n",
    "        self.input_ids      = output[\"input_ids\"]\n",
    "        self.attention_mask = output[\"attention_mask\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df.index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\":      self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_mask[idx],\n",
    "        }, self.score_counts[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset humicroedit (/home/doricirkveni/.cache/huggingface/datasets/humicroedit/subtask-1/0.0.0/209c209bc70671d8d939aefd82e51f8ff294e47504ec64ef653a93a1f13e9ed3)\n",
      "100%|██████████| 4/4 [00:00<00:00, 108.04it/s]\n"
     ]
    }
   ],
   "source": [
    "cached_sentence_embedder = SentenceEmbedder()\n",
    "cached_base_dataset = make_base_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 frames\n",
      "Dataset size: 9652\n",
      "Batch size: 512\n",
      "self.embeddings.shape=torch.Size([9652, 2, 384])\n",
      "Dataset size: 2419\n",
      "Batch size: 512\n",
      "self.embeddings.shape=torch.Size([2419, 2, 384])\n",
      "Dataset size: 3024\n",
      "Batch size: 512\n",
      "self.embeddings.shape=torch.Size([3024, 2, 384])\n"
     ]
    }
   ],
   "source": [
    "print(len(cached_base_dataset),\"frames\")\n",
    "cached_dataset_naive_sentence_embeddings = [\n",
    "    SentenceEmbeddingsDataset(\n",
    "        df, \n",
    "        precompute_embeddings(df, cached_sentence_embedder, compute_embeddings_batch_size=512, device=device),\n",
    "        device=device\n",
    "    )\n",
    "    for df in cached_base_dataset\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 9652\n",
      "Batch size: 512\n",
      "self.embeddings.shape=torch.Size([9652, 2, 384])\n",
      "Dataset size: 2419\n",
      "Batch size: 512\n",
      "self.embeddings.shape=torch.Size([2419, 2, 384])\n",
      "Dataset size: 3024\n",
      "Batch size: 512\n",
      "self.embeddings.shape=torch.Size([3024, 2, 384])\n"
     ]
    }
   ],
   "source": [
    "cached_dataset_edited_words_embeddings = [\n",
    "    SentenceEmbeddingsDataset(\n",
    "        df, \n",
    "        precompute_embeddings_focus_on_edit_word(df, cached_sentence_embedder, compute_embeddings_batch_size=512, device=device),\n",
    "        device=device\n",
    "    )\n",
    "    for df in cached_base_dataset\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>original</th>\n",
       "      <th>edit</th>\n",
       "      <th>grades</th>\n",
       "      <th>meanGrade</th>\n",
       "      <th>original_sentence</th>\n",
       "      <th>edited_sentence</th>\n",
       "      <th>original_word_start_idx</th>\n",
       "      <th>original_word_end_idx</th>\n",
       "      <th>edited_word_start_idx</th>\n",
       "      <th>edited_word_end_idx</th>\n",
       "      <th>all_scores</th>\n",
       "      <th>normalized_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14530</td>\n",
       "      <td>France is ‘ hunting down its citizens who join...</td>\n",
       "      <td>twins</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>France is ‘ hunting down its citizens who join...</td>\n",
       "      <td>France is ‘ hunting down its citizens who join...</td>\n",
       "      <td>49</td>\n",
       "      <td>53</td>\n",
       "      <td>49</td>\n",
       "      <td>54</td>\n",
       "      <td>[0, 0, 0, 0, 1]</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                           original   edit grades  \\\n",
       "0  14530  France is ‘ hunting down its citizens who join...  twins  10000   \n",
       "\n",
       "   meanGrade                                  original_sentence  \\\n",
       "0        0.2  France is ‘ hunting down its citizens who join...   \n",
       "\n",
       "                                     edited_sentence  original_word_start_idx  \\\n",
       "0  France is ‘ hunting down its citizens who join...                       49   \n",
       "\n",
       "   original_word_end_idx  edited_word_start_idx  edited_word_end_idx  \\\n",
       "0                     53                     49                   54   \n",
       "\n",
       "        all_scores  normalized_score  \n",
       "0  [0, 0, 0, 0, 1]          0.066667  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cached_dataset_naive_sentence_embeddings[0].df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cached_dataset_edited_words_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cached_dataset_edited_words_embeddings[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdf\u001b[39m.\u001b[39mdescribe()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cached_dataset_edited_words_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "cached_dataset_edited_words_embeddings[0].df.describe()\n",
    "cached_dataset_edited_words_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_dataset_tokenized_sentences = [TokenizedSentencesDataset(df, device=device) for df in cached_base_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OptimizedModule(\n",
      "  (_orig_mod): Sequential(\n",
      "    (0): Dropout(p=0.3, inplace=False)\n",
      "    (1): Linear(in_features=768, out_features=10, bias=True)\n",
      "    (2): Tanh()\n",
      "    (3): Linear(in_features=10, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-06-17 11:39:36,147] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-06-17 11:39:36,471] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.746445608139038, Val Loss: 1.6826598644256592\n",
      "Epoch 1, RMSE: 1.3215315388362996, Val RMSE: 1.297173798851048\n",
      "Epoch 11, Loss: 1.11541748046875, Val Loss: 1.6826598644256592\n",
      "Epoch 11, RMSE: 1.0561332683277949, Val RMSE: 1.297173798851048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Loss: 0.761242938041687, Val Loss: 1.6826598644256592\n",
      "Epoch 21, RMSE: 0.8724923713372438, Val RMSE: 1.297173798851048\n",
      "Epoch 31, Loss: 0.5796834230422974, Val Loss: 1.6826598644256592\n",
      "Epoch 31, RMSE: 0.7613694392621084, Val RMSE: 1.297173798851048\n",
      "Epoch 41, Loss: 0.48038743138313295, Val Loss: 1.6826598644256592\n",
      "Epoch 41, RMSE: 0.6930998711463832, Val RMSE: 1.297173798851048\n",
      "Epoch 51, Loss: 0.42594863176345826, Val Loss: 1.6826598644256592\n",
      "Epoch 51, RMSE: 0.6526474023264462, Val RMSE: 1.297173798851048\n",
      "Epoch 61, Loss: 0.3943726301193237, Val Loss: 1.6826598644256592\n",
      "Epoch 61, RMSE: 0.627990947481987, Val RMSE: 1.297173798851048\n",
      "Epoch 71, Loss: 0.37389436960220335, Val Loss: 1.6826598644256592\n",
      "Epoch 71, RMSE: 0.6114690258731045, Val RMSE: 1.297173798851048\n",
      "Epoch 81, Loss: 0.3642959713935852, Val Loss: 1.6826598644256592\n",
      "Epoch 81, RMSE: 0.6035693592235984, Val RMSE: 1.297173798851048\n",
      "Epoch 91, Loss: 0.3576738476753235, Val Loss: 1.6826598644256592\n",
      "Epoch 91, RMSE: 0.5980583982148595, Val RMSE: 1.297173798851048\n",
      "Epoch 101, Loss: 0.3537278950214386, Val Loss: 1.6826598644256592\n",
      "Epoch 101, RMSE: 0.5947502795471714, Val RMSE: 1.297173798851048\n",
      "Epoch 111, Loss: 0.3520421326160431, Val Loss: 1.6826598644256592\n",
      "Epoch 111, RMSE: 0.5933313851601338, Val RMSE: 1.297173798851048\n",
      "Epoch 121, Loss: 0.34974679350852966, Val Loss: 1.6826598644256592\n",
      "Epoch 121, RMSE: 0.5913939410482066, Val RMSE: 1.297173798851048\n",
      "Epoch 131, Loss: 0.34548150897026064, Val Loss: 1.6826598644256592\n",
      "Epoch 131, RMSE: 0.5877767509609926, Val RMSE: 1.297173798851048\n",
      "Epoch 141, Loss: 0.34348071813583375, Val Loss: 1.6826598644256592\n",
      "Epoch 141, RMSE: 0.5860722806410774, Val RMSE: 1.297173798851048\n",
      "Epoch 151, Loss: 0.34066852927207947, Val Loss: 1.6826598644256592\n",
      "Epoch 151, RMSE: 0.5836681670881833, Val RMSE: 1.297173798851048\n",
      "Epoch 161, Loss: 0.33987267017364503, Val Loss: 1.6826598644256592\n",
      "Epoch 161, RMSE: 0.582985994834906, Val RMSE: 1.297173798851048\n",
      "Epoch 171, Loss: 0.3415642619132996, Val Loss: 1.6826598644256592\n",
      "Epoch 171, RMSE: 0.5844349937446419, Val RMSE: 1.297173798851048\n",
      "Epoch 181, Loss: 0.3371260941028595, Val Loss: 1.6826598644256592\n",
      "Epoch 181, RMSE: 0.5806256057933197, Val RMSE: 1.297173798851048\n",
      "Epoch 191, Loss: 0.33675211668014526, Val Loss: 1.6826598644256592\n",
      "Epoch 191, RMSE: 0.5803034694710564, Val RMSE: 1.297173798851048\n",
      "Test Loss: 1.0116451382637024\n",
      "Test RMSE: 1.0058057159629301\n"
     ]
    }
   ],
   "source": [
    "import openpyxl\n",
    "def experiment_predict_mean_score(train_dataset, val_dataset, test_dataset):\n",
    "    batch_size   = 2048\n",
    "    num_epochs   = 200\n",
    "    lr           = 1e-4\n",
    "    weight_decay = 0\n",
    "\n",
    "\n",
    "    score_weights = torch.arange(0, 4, dtype=torch.float, device=device).unsqueeze(1)\n",
    "    def reshape_batch(batch):\n",
    "        inputs, labels = batch\n",
    "        inputs = torch.flatten(inputs, start_dim=-2)\n",
    "        labels = labels / torch.sum(labels, dim=-1, keepdim=True)\n",
    "        actual_mean_score = torch.squeeze(labels @ score_weights) / 3\n",
    "        return inputs, actual_mean_score\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(val_dataset,   batch_size=batch_size)\n",
    "    test_loader  = DataLoader(test_dataset,  batch_size=batch_size)\n",
    "\n",
    "    one_embedding, _ = train_dataset[0]\n",
    "    input_features = torch.numel(one_embedding)\n",
    "\n",
    "    layers = []\n",
    "    #while input_features > 50:\n",
    "    #    layers.append(nn.Linear(input_features, input_features // 2))\n",
    "    #    layers.append(nn.Dropout1d(0.1))\n",
    "    #    layers.append(nn.Tanh())\n",
    "    #    input_features //= 2\n",
    "    \n",
    "    layers.append(nn.Dropout(0.3))\n",
    "    layers.append(nn.Linear(input_features, 10))\n",
    "    layers.append(nn.Tanh())\n",
    "    layers.append(nn.Linear(10, 1))\n",
    "    #layers.append(nn.Sigmoid())\n",
    "\n",
    "    model = nn.Sequential(*layers)\n",
    "    model = model.to(device)\n",
    "    model = torch.compile(model)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    print(model)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = reshape_batch(data)\n",
    "            labels *= 3\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(val_loader, 0):\n",
    "                inputs, labels = reshape_batch(data)\n",
    "                labels *= 3\n",
    "                outputs = model(inputs).squeeze()\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}, Val Loss: {val_loss/len(val_loader)}\")\n",
    "            print(f\"Epoch {epoch+1}, RMSE: {np.sqrt(running_loss/len(train_loader))}, Val RMSE: {np.sqrt(val_loss/len(val_loader))}\")\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader, 0):\n",
    "            inputs, labels = reshape_batch(data)\n",
    "            labels *= 3\n",
    "            outputs = model(inputs).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "    print(f\"Test Loss: {test_loss/len(test_loader)}\")\n",
    "    print(f\"Test RMSE: {np.sqrt(test_loss/len(test_loader))}\")\n",
    "\n",
    "    # Excel output\n",
    "    predicted_scores = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader, 0):\n",
    "            inputs, labels = reshape_batch(data)\n",
    "            labels *= 3\n",
    "            outputs = model(inputs).squeeze()\n",
    "            predicted_scores.extend(outputs.tolist())\n",
    "            \n",
    "    excel_df = test_dataset.df.copy()\n",
    "    excel_df[\"normalized_predicted_score\"] = predicted_scores\n",
    "    excel_df.to_excel(\"experiment_predict_mean_score.xlsx\")\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch._dynamo.reset()\n",
    "\n",
    "experiment_predict_mean_score(*cached_dataset_naive_sentence_embeddings)\n",
    "#experiment_predict_mean_score(*cached_dataset_edited_words_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-06-17 11:40:16,256] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OptimizedModule(\n",
      "  (_orig_mod): Sequential(\n",
      "    (0): Dropout(p=0.3, inplace=False)\n",
      "    (1): Linear(in_features=768, out_features=4, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-06-17 11:40:16,505] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.40889630317688, Val Loss: 1.4002351760864258\n",
      "Epoch 11, Loss: 1.3280177593231202, Val Loss: 1.4002351760864258\n",
      "Epoch 21, Loss: 1.292422342300415, Val Loss: 1.4002351760864258\n",
      "Epoch 31, Loss: 1.2776560306549072, Val Loss: 1.4002351760864258\n",
      "Epoch 41, Loss: 1.2686731100082398, Val Loss: 1.4002351760864258\n",
      "Epoch 51, Loss: 1.2627175331115723, Val Loss: 1.4002351760864258\n",
      "Epoch 61, Loss: 1.2589459657669066, Val Loss: 1.4002351760864258\n",
      "Epoch 71, Loss: 1.256398367881775, Val Loss: 1.4002351760864258\n",
      "Epoch 81, Loss: 1.2529144287109375, Val Loss: 1.4002351760864258\n",
      "Epoch 91, Loss: 1.2506630420684814, Val Loss: 1.4002351760864258\n",
      "Epoch 101, Loss: 1.249015474319458, Val Loss: 1.4002351760864258\n",
      "Epoch 111, Loss: 1.246162724494934, Val Loss: 1.4002351760864258\n",
      "Epoch 121, Loss: 1.24592022895813, Val Loss: 1.4002351760864258\n",
      "Epoch 131, Loss: 1.244282078742981, Val Loss: 1.4002351760864258\n",
      "Epoch 141, Loss: 1.2427595138549805, Val Loss: 1.4002351760864258\n",
      "Epoch 151, Loss: 1.2422583103179932, Val Loss: 1.4002351760864258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161, Loss: 1.24182870388031, Val Loss: 1.4002351760864258\n",
      "Epoch 171, Loss: 1.239982295036316, Val Loss: 1.4002351760864258\n",
      "Epoch 181, Loss: 1.239652395248413, Val Loss: 1.4002351760864258\n",
      "Epoch 191, Loss: 1.2382166624069213, Val Loss: 1.4002351760864258\n",
      "Epoch 201, Loss: 1.2376933813095092, Val Loss: 1.4002351760864258\n",
      "Epoch 211, Loss: 1.2387736082077025, Val Loss: 1.4002351760864258\n",
      "Epoch 221, Loss: 1.23733332157135, Val Loss: 1.4002351760864258\n",
      "Epoch 231, Loss: 1.2363936185836792, Val Loss: 1.4002351760864258\n",
      "Epoch 241, Loss: 1.235958194732666, Val Loss: 1.4002351760864258\n",
      "Epoch 251, Loss: 1.2373943567276, Val Loss: 1.4002351760864258\n",
      "Epoch 261, Loss: 1.234455132484436, Val Loss: 1.4002351760864258\n",
      "Epoch 271, Loss: 1.2344252347946167, Val Loss: 1.4002351760864258\n",
      "Epoch 281, Loss: 1.2336561679840088, Val Loss: 1.4002351760864258\n",
      "Epoch 291, Loss: 1.2328949451446534, Val Loss: 1.4002351760864258\n",
      "Epoch 301, Loss: 1.2337469816207887, Val Loss: 1.4002351760864258\n",
      "Epoch 311, Loss: 1.2343360424041747, Val Loss: 1.4002351760864258\n",
      "Epoch 321, Loss: 1.2323934078216552, Val Loss: 1.4002351760864258\n",
      "Epoch 331, Loss: 1.2322755575180053, Val Loss: 1.4002351760864258\n",
      "Epoch 341, Loss: 1.2322163343429566, Val Loss: 1.4002351760864258\n",
      "Epoch 351, Loss: 1.2318015098571777, Val Loss: 1.4002351760864258\n",
      "Epoch 361, Loss: 1.2306724786758423, Val Loss: 1.4002351760864258\n",
      "Epoch 371, Loss: 1.2311865568161011, Val Loss: 1.4002351760864258\n",
      "Epoch 381, Loss: 1.231300163269043, Val Loss: 1.4002351760864258\n",
      "Epoch 391, Loss: 1.2300815820693969, Val Loss: 1.4002351760864258\n",
      "Epoch 401, Loss: 1.2308687686920166, Val Loss: 1.4002351760864258\n",
      "Epoch 411, Loss: 1.2304752349853516, Val Loss: 1.4002351760864258\n",
      "Epoch 421, Loss: 1.2307077407836915, Val Loss: 1.4002351760864258\n",
      "Epoch 431, Loss: 1.2296694993972779, Val Loss: 1.4002351760864258\n",
      "Epoch 441, Loss: 1.2297288417816161, Val Loss: 1.4002351760864258\n",
      "Epoch 451, Loss: 1.2293027877807616, Val Loss: 1.4002351760864258\n",
      "Epoch 461, Loss: 1.2287570714950562, Val Loss: 1.4002351760864258\n",
      "Epoch 471, Loss: 1.2280554533004762, Val Loss: 1.4002351760864258\n",
      "Epoch 481, Loss: 1.2286238193511962, Val Loss: 1.4002351760864258\n",
      "Epoch 491, Loss: 1.2294330835342406, Val Loss: 1.4002351760864258\n",
      "Test Loss: 1.32161283493042\n"
     ]
    }
   ],
   "source": [
    "def experiment_predict_score_distribution(train_dataset, val_dataset, test_dataset):\n",
    "    batch_size   = 2048\n",
    "    num_epochs   = 500\n",
    "    lr           = 1e-4\n",
    "    weight_decay = 0\n",
    "\n",
    "    def reshape_batch(batch):\n",
    "        inputs, labels = batch\n",
    "        inputs = torch.flatten(inputs, start_dim=-2)\n",
    "        labels = labels / torch.sum(labels, dim=-1, keepdim=True)\n",
    "        return inputs, labels\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(val_dataset,   batch_size=batch_size)\n",
    "    test_loader  = DataLoader(test_dataset,  batch_size=batch_size)\n",
    "\n",
    "    one_embedding, _ = train_dataset[0]\n",
    "    input_features = torch.numel(one_embedding)\n",
    "\n",
    "    layers = []\n",
    "    #while input_features > 50:\n",
    "    #    layers.append(nn.Linear(input_features, input_features // 2))\n",
    "    #    layers.append(nn.Dropout1d(0.1))\n",
    "    #    layers.append(nn.Tanh())\n",
    "    #    input_features //= 2\n",
    "    \n",
    "    layers.append(nn.Dropout(0.3))\n",
    "    layers.append(nn.Linear(input_features, 4))\n",
    "    #layers.append(nn.Tanh())\n",
    "    #layers.append(nn.Linear(10, 4)) # logits\n",
    "\n",
    "    model = nn.Sequential(*layers)\n",
    "    model = torch.compile(model.to(device))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    print(model)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = reshape_batch(data)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for i, data in enumerate(val_loader, 0):\n",
    "                    inputs, labels = reshape_batch(data)\n",
    "                    outputs = model(inputs).squeeze()\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}, Val Loss: {val_loss/len(val_loader)}\")\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader, 0):\n",
    "            inputs, labels = reshape_batch(data)\n",
    "            outputs = model(inputs).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "    print(f\"Test Loss: {test_loss/len(test_loader)}\")\n",
    "\n",
    "    # Excel output\n",
    "    predicted_scores = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader, 0):\n",
    "            inputs, labels = reshape_batch(data)\n",
    "            logits = model(inputs).squeeze()\n",
    "\n",
    "            p = torch.softmax(logits, dim=1)\n",
    "            score_weights = torch.arange(0, 4, dtype=torch.float, device=device).unsqueeze(1)\n",
    "            mean_score = torch.squeeze(p @ score_weights) / 3\n",
    "            predicted_scores.extend(mean_score.tolist())\n",
    "            \n",
    "    excel_df = test_dataset.df.copy()\n",
    "    excel_df[\"normalized_predicted_score\"] = predicted_scores\n",
    "    excel_df.to_excel(\"experiment_predict_score_distribution.xlsx\")\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch._dynamo.reset()\n",
    "\n",
    "#experiment_predict_score_distribution(*cached_dataset_edited_words_embeddings)\n",
    "experiment_predict_score_distribution(*cached_dataset_naive_sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_predict_score_distribution_finetune_distillbert(train_dataset, val_dataset, test_dataset):\n",
    "    batch_size   = 16 \n",
    "    num_epochs   = 1000\n",
    "    lr           = 1e-4\n",
    "    lr_bert      = 3e-5\n",
    "    weight_decay = 0\n",
    "\n",
    "    def reshape_batch(batch):\n",
    "        inputs, labels = batch\n",
    "        labels = labels / torch.sum(labels, dim=-1, keepdim=True)\n",
    "        return inputs, labels\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(val_dataset,   batch_size=batch_size)\n",
    "    test_loader  = DataLoader(test_dataset,  batch_size=batch_size)\n",
    "\n",
    "    distilbert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "    distilbert = distilbert.to(device)\n",
    "    #distilbert = torch.compile(distilbert)\n",
    "\n",
    "    for param in distilbert.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    layers = [\n",
    "              nn.Linear(768, 768),\n",
    "              nn.Dropout1d(0.3),\n",
    "              nn.Tanh(),\n",
    "              nn.Linear(768, 4)] # logits\n",
    "\n",
    "    model = nn.Sequential(*layers)\n",
    "    model = torch.compile(model.to(device))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam([\n",
    "        { \"params\": distilbert.parameters(), \"lr\": lr_bert },\n",
    "        { \"params\": model.parameters(),      \"lr\": lr, \"weight_decay\": weight_decay },\n",
    "    ])\n",
    "\n",
    "    print(model)\n",
    "\n",
    "    def logits_from_inputs(distilbert, model, inputs):\n",
    "        outputs = distilbert(**inputs).last_hidden_state[:, 0, :]\n",
    "        logits = model(outputs).squeeze()\n",
    "        return logits\n",
    "        \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = reshape_batch(data)\n",
    "            logits = logits_from_inputs(distilbert, model, inputs)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for i, data in enumerate(val_loader, 0):\n",
    "                    inputs, labels = reshape_batch(data)\n",
    "                    logits = logits_from_inputs(distilbert, model, inputs)\n",
    "                    loss = criterion(logits, labels)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}, Val Loss: {val_loss/len(val_loader)}\")\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader, 0):\n",
    "            inputs, labels = reshape_batch(data)\n",
    "            logits = logits_from_inputs(distilbert, model, inputs)\n",
    "            loss = criterion(logits, labels)\n",
    "            test_loss += loss.item()\n",
    "    print(f\"Test Loss: {test_loss/len(test_loader)}\")\n",
    "\n",
    "    # evaluate the model and print the RMS loss on the test set\n",
    "    model.eval()\n",
    "    mse_loss = torch.nn.MSELoss()\n",
    "    with torch.no_grad():\n",
    "        predictions = []\n",
    "        actuals = []\n",
    "        for i, data in enumerate(test_loader, 0):\n",
    "            inputs, labels = reshape_batch(data)\n",
    "            logits = logits_from_inputs(distilbert, model, inputs)\n",
    "            predictions.extend(torch.argmax(logits, dim=1).tolist())\n",
    "            actuals.extend(torch.argmax(labels, dim=1).tolist())\n",
    "        print(f\"Test Accuracy: {metric.compute(predictions=predictions, references=actuals)}\")\n",
    "\n",
    "    # Excel output\n",
    "    predicted_scores = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader, 0):\n",
    "            inputs, labels = reshape_batch(data)\n",
    "            logits = logits_from_inputs(distilbert, model, inputs)\n",
    "\n",
    "            p = torch.softmax(logits, dim=1)\n",
    "            score_weights = torch.arange(0, 4, dtype=torch.float, device=device).unsqueeze(1)\n",
    "            mean_score = torch.squeeze(p @ score_weights) / 3\n",
    "            predicted_scores.extend(mean_score.tolist())\n",
    "            \n",
    "    excel_df = test_dataset.df.copy()\n",
    "    excel_df[\"normalized_predicted_score\"] = predicted_scores\n",
    "    excel_df.to_excel(\"experiment_predict_score_distribution_finetune_distillbert.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OptimizedModule(\n",
      "  (_orig_mod): Sequential(\n",
      "    (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (1): Dropout1d(p=0.3, inplace=False)\n",
      "    (2): Tanh()\n",
      "    (3): Linear(in_features=768, out_features=4, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-06-17 11:41:23,498] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-06-17 11:49:16,382] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 3.2340024648517964, Val Loss: 9.326806482515837\n",
      "Epoch 2, Loss: 6.819482693609023, Val Loss: 9.326806482515837\n",
      "Epoch 3, Loss: 7.4665414467947375, Val Loss: 9.326806482515837\n",
      "Epoch 4, Loss: 11.432153685598184, Val Loss: 9.326806482515837\n",
      "Epoch 5, Loss: 9.454585294455093, Val Loss: 9.326806482515837\n",
      "Epoch 6, Loss: 10.859023398910926, Val Loss: 9.326806482515837\n",
      "Epoch 7, Loss: 11.690430543675328, Val Loss: 9.326806482515837\n",
      "Epoch 8, Loss: 13.909496271057634, Val Loss: 9.326806482515837\n",
      "Epoch 9, Loss: 11.98361522709297, Val Loss: 9.326806482515837\n",
      "Epoch 10, Loss: 13.946775042458086, Val Loss: 9.326806482515837\n",
      "Epoch 11, Loss: 15.828599956651397, Val Loss: 9.326806482515837\n",
      "Epoch 12, Loss: 11.679380189898788, Val Loss: 9.326806482515837\n",
      "Epoch 13, Loss: 13.986621072355485, Val Loss: 9.326806482515837\n",
      "Epoch 14, Loss: 11.721322388443726, Val Loss: 9.326806482515837\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n\u001b[1;32m      2\u001b[0m torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mreset()\n\u001b[0;32m----> 3\u001b[0m experiment_predict_score_distribution_finetune_distillbert(\u001b[39m*\u001b[39;49mcached_dataset_tokenized_sentences)\n",
      "Cell \u001b[0;32mIn[16], line 50\u001b[0m, in \u001b[0;36mexperiment_predict_score_distribution_finetune_distillbert\u001b[0;34m(train_dataset, val_dataset, test_dataset)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39mfor\u001b[39;00m i, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader, \u001b[39m0\u001b[39m):\n\u001b[1;32m     49\u001b[0m     inputs, labels \u001b[39m=\u001b[39m reshape_batch(data)\n\u001b[0;32m---> 50\u001b[0m     logits \u001b[39m=\u001b[39m logits_from_inputs(distilbert, model, inputs)\n\u001b[1;32m     51\u001b[0m     loss \u001b[39m=\u001b[39m criterion(logits, labels)\n\u001b[1;32m     52\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n",
      "Cell \u001b[0;32mIn[16], line 41\u001b[0m, in \u001b[0;36mexperiment_predict_score_distribution_finetune_distillbert.<locals>.logits_from_inputs\u001b[0;34m(distilbert, model, inputs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlogits_from_inputs\u001b[39m(distilbert, model, inputs):\n\u001b[0;32m---> 41\u001b[0m     outputs \u001b[39m=\u001b[39m distilbert(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\u001b[39m.\u001b[39mlast_hidden_state[:, \u001b[39m0\u001b[39m, :]\n\u001b[1;32m     42\u001b[0m     logits \u001b[39m=\u001b[39m model(outputs)\u001b[39m.\u001b[39msqueeze()\n\u001b[1;32m     43\u001b[0m     \u001b[39mreturn\u001b[39;00m logits\n",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py:583\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    579\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    581\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(input_ids, inputs_embeds)  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m--> 583\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m    584\u001b[0m     x\u001b[39m=\u001b[39;49membeddings,\n\u001b[1;32m    585\u001b[0m     attn_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    586\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    587\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    588\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    589\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    590\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py:359\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[39mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    357\u001b[0m     all_hidden_states \u001b[39m=\u001b[39m all_hidden_states \u001b[39m+\u001b[39m (hidden_state,)\n\u001b[0;32m--> 359\u001b[0m layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    360\u001b[0m     x\u001b[39m=\u001b[39;49mhidden_state, attn_mask\u001b[39m=\u001b[39;49mattn_mask, head_mask\u001b[39m=\u001b[39;49mhead_mask[i], output_attentions\u001b[39m=\u001b[39;49moutput_attentions\n\u001b[1;32m    361\u001b[0m )\n\u001b[1;32m    362\u001b[0m hidden_state \u001b[39m=\u001b[39m layer_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    364\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py:313\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    310\u001b[0m sa_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msa_layer_norm(sa_output \u001b[39m+\u001b[39m x)  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[39m# Feed Forward Network\u001b[39;00m\n\u001b[0;32m--> 313\u001b[0m ffn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mffn(sa_output)  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m    314\u001b[0m ffn_output: torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_layer_norm(ffn_output \u001b[39m+\u001b[39m sa_output)  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m    316\u001b[0m output \u001b[39m=\u001b[39m (ffn_output,)\n",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py:254\u001b[0m, in \u001b[0;36mFFN.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 254\u001b[0m     \u001b[39mreturn\u001b[39;00m apply_chunking_to_forward(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mff_chunk, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchunk_size_feed_forward, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseq_len_dim, \u001b[39minput\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/site-packages/transformers/pytorch_utils.py:237\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[39m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(output_chunks, dim\u001b[39m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 237\u001b[0m \u001b[39mreturn\u001b[39;00m forward_fn(\u001b[39m*\u001b[39;49minput_tensors)\n",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py:257\u001b[0m, in \u001b[0;36mFFN.ff_chunk\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mff_chunk\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 257\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlin1(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    258\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation(x)\n\u001b[1;32m    259\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlin2(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch._dynamo.reset()\n",
    "experiment_predict_score_distribution_finetune_distillbert(*cached_dataset_tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_predict_mean_score_finetune_distilbert_huggingface(train_df, val_df, test_df):\n",
    "    train_dataset = TokenizedSentencesDataset(train_df, device=\"cpu\")\n",
    "    val_dataset   = TokenizedSentencesDataset(val_df,   device=\"cpu\")\n",
    "    test_dataset  = TokenizedSentencesDataset(test_df,  device=\"cpu\")\n",
    "\n",
    "    batch_size   = 16\n",
    "    num_epochs   = 4\n",
    "    lr           = 5e-5\n",
    "    weight_decay = 0.0001\n",
    "\n",
    "    class NormalizedLabelsDataset(Dataset):\n",
    "        def __init__(self, base_dataset):\n",
    "            self.base_dataset = base_dataset\n",
    "        def __len__(self):\n",
    "            return len(self.base_dataset)\n",
    "        def __getitem__(self, idx):\n",
    "            inputs, labels = self.base_dataset[idx]\n",
    "            labels = labels / torch.sum(labels, dim=-1, keepdim=True)\n",
    "            score_weights     = torch.arange(0, 4, dtype=torch.float).unsqueeze(1)\n",
    "            actual_mean_score = torch.squeeze(labels @ score_weights) / 3\n",
    "            return { \"labels\": actual_mean_score, **inputs }\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased', use_fast=True)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=1)\n",
    "    #model = torch.compile(model.to(device))\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        \"distilbert-base-uncased-finetuned-subtask-1\",\n",
    "        evaluation_strategy = \"steps\",\n",
    "        eval_steps=200,\n",
    "        save_strategy = \"steps\",\n",
    "        save_steps=200,\n",
    "        learning_rate=lr,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=num_epochs,\n",
    "        weight_decay=weight_decay,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"mse\",\n",
    "    )\n",
    "\n",
    "    mae_metric = evaluate.load(\"mae\")\n",
    "    mse_metric = evaluate.load(\"mse\")\n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        predictions, labels = torch.tensor(predictions),  torch.tensor(labels)\n",
    "        predictions, labels = torch.squeeze(predictions), torch.squeeze(labels)\n",
    "        predictions *= 3\n",
    "        labels      *= 3\n",
    "        \n",
    "        #p = torch.softmax(predictions, dim=1)\n",
    "        #score_weights = torch.arange(0, 4, dtype=torch.float).unsqueeze(1)\n",
    "        #predicted_mean_score = torch.squeeze(p      @ score_weights) / 3\n",
    "        #actual_mean_score    = torch.squeeze(labels @ score_weights) / 3\n",
    "        #return metric.compute(predictions=predicted_mean_score, references=actual_mean_score)\n",
    "        mse = mse_metric.compute(predictions=predictions, references=labels)\n",
    "        mae = mae_metric.compute(predictions=predictions, references=labels)\n",
    "        return {\n",
    "            \"mae\":  mae[\"mae\"],\n",
    "            \"mse\":  mse[\"mse\"],\n",
    "            \"rmse\": np.sqrt(mse[\"mse\"]),\n",
    "        }\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        args,\n",
    "        train_dataset=NormalizedLabelsDataset(train_dataset),\n",
    "        eval_dataset=NormalizedLabelsDataset(val_dataset),\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.evaluate()\n",
    "    out = trainer.predict(NormalizedLabelsDataset(test_dataset))\n",
    "    predictions = np.squeeze(out.predictions)\n",
    "    # compute metrics\n",
    "    predictions *= 3\n",
    "    \n",
    "\n",
    "    excel_df = test_dataset.df.copy()\n",
    "    excel_df[\"normalized_predicted_score\"] = predictions\n",
    "    excel_df.to_excel(\"experiment_predict_mean_score_finetune_distillbert.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n\u001b[1;32m      2\u001b[0m torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mreset()\n\u001b[0;32m----> 4\u001b[0m experiment_predict_mean_score_finetune_distilbert_huggingface(\u001b[39m*\u001b[39;49mcached_base_dataset)\n",
      "Cell \u001b[0;32mIn[13], line 24\u001b[0m, in \u001b[0;36mexperiment_predict_mean_score_finetune_distilbert_huggingface\u001b[0;34m(train_df, val_df, test_df)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[39mreturn\u001b[39;00m { \u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m: actual_mean_score, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs }\n\u001b[1;32m     23\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mdistilbert-base-uncased\u001b[39m\u001b[39m'\u001b[39m, use_fast\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 24\u001b[0m model \u001b[39m=\u001b[39m AutoModelForSequenceClassification\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m'\u001b[39;49m\u001b[39mdistilbert-base-uncased\u001b[39;49m\u001b[39m'\u001b[39;49m, num_labels\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     25\u001b[0m \u001b[39m#model = torch.compile(model.to(device))\u001b[39;00m\n\u001b[1;32m     27\u001b[0m args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[1;32m     28\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mdistilbert-base-uncased-finetuned-subtask-1\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     29\u001b[0m     evaluation_strategy \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msteps\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m     metric_for_best_model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmse\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     40\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:484\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    483\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 484\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    485\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    486\u001b[0m     )\n\u001b[1;32m    487\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    488\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    489\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/site-packages/transformers/modeling_utils.py:2881\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2871\u001b[0m     \u001b[39mif\u001b[39;00m dtype_orig \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2872\u001b[0m         torch\u001b[39m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   2874\u001b[0m     (\n\u001b[1;32m   2875\u001b[0m         model,\n\u001b[1;32m   2876\u001b[0m         missing_keys,\n\u001b[1;32m   2877\u001b[0m         unexpected_keys,\n\u001b[1;32m   2878\u001b[0m         mismatched_keys,\n\u001b[1;32m   2879\u001b[0m         offload_index,\n\u001b[1;32m   2880\u001b[0m         error_msgs,\n\u001b[0;32m-> 2881\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_load_pretrained_model(\n\u001b[1;32m   2882\u001b[0m         model,\n\u001b[1;32m   2883\u001b[0m         state_dict,\n\u001b[1;32m   2884\u001b[0m         loaded_state_dict_keys,  \u001b[39m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   2885\u001b[0m         resolved_archive_file,\n\u001b[1;32m   2886\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   2887\u001b[0m         ignore_mismatched_sizes\u001b[39m=\u001b[39;49mignore_mismatched_sizes,\n\u001b[1;32m   2888\u001b[0m         sharded_metadata\u001b[39m=\u001b[39;49msharded_metadata,\n\u001b[1;32m   2889\u001b[0m         _fast_init\u001b[39m=\u001b[39;49m_fast_init,\n\u001b[1;32m   2890\u001b[0m         low_cpu_mem_usage\u001b[39m=\u001b[39;49mlow_cpu_mem_usage,\n\u001b[1;32m   2891\u001b[0m         device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m   2892\u001b[0m         offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[1;32m   2893\u001b[0m         offload_state_dict\u001b[39m=\u001b[39;49moffload_state_dict,\n\u001b[1;32m   2894\u001b[0m         dtype\u001b[39m=\u001b[39;49mtorch_dtype,\n\u001b[1;32m   2895\u001b[0m         is_quantized\u001b[39m=\u001b[39;49m(load_in_8bit \u001b[39mor\u001b[39;49;00m load_in_4bit),\n\u001b[1;32m   2896\u001b[0m         keep_in_fp32_modules\u001b[39m=\u001b[39;49mkeep_in_fp32_modules,\n\u001b[1;32m   2897\u001b[0m     )\n\u001b[1;32m   2899\u001b[0m model\u001b[39m.\u001b[39mis_loaded_in_4bit \u001b[39m=\u001b[39m load_in_4bit\n\u001b[1;32m   2900\u001b[0m model\u001b[39m.\u001b[39mis_loaded_in_8bit \u001b[39m=\u001b[39m load_in_8bit\n",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/site-packages/transformers/modeling_utils.py:3182\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   3172\u001b[0m \u001b[39mif\u001b[39;00m state_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3173\u001b[0m     \u001b[39m# Whole checkpoint\u001b[39;00m\n\u001b[1;32m   3174\u001b[0m     mismatched_keys \u001b[39m=\u001b[39m _find_mismatched_keys(\n\u001b[1;32m   3175\u001b[0m         state_dict,\n\u001b[1;32m   3176\u001b[0m         model_state_dict,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3180\u001b[0m         ignore_mismatched_sizes,\n\u001b[1;32m   3181\u001b[0m     )\n\u001b[0;32m-> 3182\u001b[0m     error_msgs \u001b[39m=\u001b[39m _load_state_dict_into_model(model_to_load, state_dict, start_prefix)\n\u001b[1;32m   3183\u001b[0m     offload_index \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   3184\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3185\u001b[0m     \u001b[39m# Sharded checkpoint or whole but low_cpu_mem_usage==True\u001b[39;00m\n\u001b[1;32m   3186\u001b[0m \n\u001b[1;32m   3187\u001b[0m     \u001b[39m# This should always be a list but, just to be sure.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/site-packages/transformers/modeling_utils.py:550\u001b[0m, in \u001b[0;36m_load_state_dict_into_model\u001b[0;34m(model_to_load, state_dict, start_prefix)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[39mif\u001b[39;00m child \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    548\u001b[0m             load(child, state_dict, prefix \u001b[39m+\u001b[39m name \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 550\u001b[0m load(model_to_load, state_dict, prefix\u001b[39m=\u001b[39;49mstart_prefix)\n\u001b[1;32m    551\u001b[0m \u001b[39m# Delete `state_dict` so it could be collected by GC earlier. Note that `state_dict` is a copy of the argument, so\u001b[39;00m\n\u001b[1;32m    552\u001b[0m \u001b[39m# it's safe to delete it.\u001b[39;00m\n\u001b[1;32m    553\u001b[0m \u001b[39mdel\u001b[39;00m state_dict\n",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/site-packages/transformers/modeling_utils.py:548\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[39mfor\u001b[39;00m name, child \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39m_modules\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    547\u001b[0m     \u001b[39mif\u001b[39;00m child \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 548\u001b[0m         load(child, state_dict, prefix \u001b[39m+\u001b[39;49m name \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/site-packages/transformers/modeling_utils.py:548\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[39mfor\u001b[39;00m name, child \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39m_modules\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    547\u001b[0m     \u001b[39mif\u001b[39;00m child \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 548\u001b[0m         load(child, state_dict, prefix \u001b[39m+\u001b[39;49m name \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "    \u001b[0;31m[... skipping similar frames: _load_state_dict_into_model.<locals>.load at line 548 (3 times)]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/site-packages/transformers/modeling_utils.py:548\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[39mfor\u001b[39;00m name, child \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39m_modules\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    547\u001b[0m     \u001b[39mif\u001b[39;00m child \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 548\u001b[0m         load(child, state_dict, prefix \u001b[39m+\u001b[39;49m name \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/site-packages/transformers/modeling_utils.py:544\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    542\u001b[0m                     module\u001b[39m.\u001b[39m_load_from_state_dict(\u001b[39m*\u001b[39margs)\n\u001b[1;32m    543\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 544\u001b[0m         module\u001b[39m.\u001b[39;49m_load_from_state_dict(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    546\u001b[0m \u001b[39mfor\u001b[39;00m name, child \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39m_modules\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    547\u001b[0m     \u001b[39mif\u001b[39;00m child \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/tpenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1942\u001b[0m, in \u001b[0;36mModule._load_from_state_dict\u001b[0;34m(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\u001b[0m\n\u001b[1;32m   1940\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1941\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m-> 1942\u001b[0m         param\u001b[39m.\u001b[39;49mcopy_(input_param)\n\u001b[1;32m   1943\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m ex:\n\u001b[1;32m   1944\u001b[0m     error_msgs\u001b[39m.\u001b[39mappend(\u001b[39m'\u001b[39m\u001b[39mWhile copying the parameter named \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1945\u001b[0m                       \u001b[39m'\u001b[39m\u001b[39mwhose dimensions in the model are \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1946\u001b[0m                       \u001b[39m'\u001b[39m\u001b[39mwhose dimensions in the checkpoint are \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1947\u001b[0m                       \u001b[39m'\u001b[39m\u001b[39man exception occurred : \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1948\u001b[0m                       \u001b[39m.\u001b[39mformat(key, param\u001b[39m.\u001b[39msize(), input_param\u001b[39m.\u001b[39msize(), ex\u001b[39m.\u001b[39margs))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch._dynamo.reset()\n",
    "\n",
    "experiment_predict_mean_score_finetune_distilbert_huggingface(*cached_base_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading model.safetensors:  31%|███▏      | 83.9M/268M [12:22<27:09, 113kB/s] \n",
      "Downloading model.safetensors:  63%|██████▎   | 168M/268M [09:09<05:28, 305kB/s] \n",
      "Downloading model.safetensors:  27%|██▋       | 73.4M/268M [06:33<17:22, 187kB/s] \n",
      "Downloading (…)e9125/.gitattributes: 100%|██████████| 1.18k/1.18k [00:00<00:00, 10.8kB/s]\n",
      "Downloading (…)_Pooling/config.json: 100%|██████████| 190/190 [00:00<00:00, 55.7kB/s]\n",
      "Downloading (…)7e55de9125/README.md: 100%|██████████| 10.6k/10.6k [00:00<00:00, 6.70MB/s]\n",
      "Downloading (…)55de9125/config.json: 100%|██████████| 612/612 [00:00<00:00, 151kB/s]\n",
      "Downloading (…)ce_transformers.json: 100%|██████████| 116/116 [00:00<00:00, 42.8kB/s]\n",
      "Downloading (…)125/data_config.json: 100%|██████████| 39.3k/39.3k [00:00<00:00, 1.36MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 90.9M/90.9M [00:27<00:00, 3.28MB/s]\n",
      "Downloading (…)nce_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 31.9kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 41.8kB/s]\n",
      "Downloading (…)e9125/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 2.05MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 350/350 [00:00<00:00, 59.0kB/s]\n",
      "Downloading (…)9125/train_script.py: 100%|██████████| 13.2k/13.2k [00:00<00:00, 3.78MB/s]\n",
      "Downloading (…)7e55de9125/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 1.92MB/s]\n",
      "Downloading (…)5de9125/modules.json: 100%|██████████| 349/349 [00:00<00:00, 92.5kB/s]\n",
      "Iteration: 100%|██████████| 1/1 [00:05<00:00,  5.73s/it]\n",
      "Epoch: 100%|██████████| 1/1 [00:05<00:00,  5.74s/it]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import openai\n",
    "openai.api_key = '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'\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, LoggingHandler, losses, InputExample\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def make_base_dataset():\n",
    "    dataset = load_dataset(\"humicroedit\", \"subtask-1\")\n",
    "\n",
    "    train_df = pd.DataFrame(dataset[\"train\"])\n",
    "    val_df   = pd.DataFrame(dataset[\"validation\"])\n",
    "    test_df  = pd.DataFrame(dataset[\"test\"])\n",
    "\n",
    "    dfs = [train_df, test_df, val_df]\n",
    "    for df in dfs:\n",
    "        def edit_the_headline(original, edit):\n",
    "            openIdx  = original.index(\"<\")\n",
    "            closeIdx = original.index(\"/>\") + len(\"/>\")\n",
    "            return original[:openIdx] + edit + original[closeIdx:]\n",
    "        \n",
    "        df[\"original_sentence\"] = df[\"original\"].apply(lambda s: s.replace(\"<\", \"\").replace(\"/>\", \"\"))\n",
    "        df[\"edited_sentence\"]   = df.apply(lambda row: edit_the_headline(row[\"original\"], row[\"edit\"]), axis=1)\n",
    "\n",
    "        df[\"original_word_start_idx\"] = df[\"original\"].apply(lambda s: s.index(\"<\"))        \n",
    "        df[\"original_word_end_idx\"]   = df[\"original\"].apply(lambda s: s.index(\"/>\") - 1)\n",
    "\n",
    "        df[\"edited_word_start_idx\"] = df[\"original\"].apply(lambda s: s.index(\"<\"))\n",
    "        df[\"edited_word_end_idx\"]   = df.apply(lambda row: row[\"edited_word_start_idx\"] + len(row[\"edit\"]), axis=1)\n",
    "\n",
    "        df[\"all_scores\"]       = df[\"grades\"].apply(lambda s: sorted([int(c) for c in s]))\n",
    "        df[\"normalized_score\"] = df[\"meanGrade\"] / 3.0\n",
    "\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "train_examples = [\n",
    "    InputExample(texts=['This is a positive pair', 'Where the distance will be minimized'], label=1),\n",
    "    InputExample(texts=['This is a negative pair', 'Their distance will be increased'], label=0)]\n",
    "\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=2)\n",
    "train_loss = losses.ContrastiveLoss(model=model)\n",
    "\n",
    "model.fit([(train_dataloader, train_loss)], show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import openai\n",
    "openai.api_key = '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'\n",
    "\n",
    "train_df = cached_base_dataset[0]\n",
    "# random sample of 10 from train_df\n",
    "\n",
    "top_df = train_df.sort_values(by=[\"meanGrade\"], ascending=True).head(50).sample(5)\n",
    "bot_df = train_df.sort_values(by=[\"meanGrade\"], ascending=False).head(50).sample(5)\n",
    "\n",
    "a = list(train_df[\"original\"].sample(10)) + list(top_df[\"original\"]) + list(bot_df[\"original\"])\n",
    "b = list(train_df[\"edit\"].sample(10))     + list(top_df[\"edit\"])     + list(bot_df[\"edit\"])\n",
    "for aa, bb in zip(a, b):\n",
    "    aa = aa.replace(\"<\", \"[ \").replace(\"/>\", f\" => {bb} ]\")\n",
    "    print(aa)\n",
    "\n",
    "def generate_explanations(df, filename, completions_per_headline=1):\n",
    "    prompt = \"\"\"\n",
    "        The following news headlines have been edited to be more humorous.\n",
    "        The format of the headline is \"text text [[ original word => edited word ]] text text\".\n",
    "        Explain what kind of humorous response the edit wanted to elicit, and wether it suceeeded or fell flat.\n",
    "        You are not to be too easily offended. Answer as concisely as possible. When explaining something refer to the exact part in the headline.\n",
    "        Do not use more than 3 sentences. Only output the explanation, nothing else.\n",
    "\n",
    "        Headline:\n",
    "        REPLACE_WITH_HEADLINE\n",
    "    \"\"\"\n",
    "\n",
    "    completions = defaultdict(list)\n",
    "    for i in range(len(df.index)):\n",
    "        headline = df.iloc[i]\n",
    "        original = headline[\"original\"]\n",
    "        edit     = headline[\"edit\"]\n",
    "        combined = original.replace(\"<\", \"[[ \").replace(\"/>\", f\" => {edit} ]]\")\n",
    "        \n",
    "        this_prompt = prompt.replace(\"REPLACE_WITH_HEADLINE\", combined)\n",
    "\n",
    "        print(\"generating\", i, \"/\", len(df.index))\n",
    "\n",
    "        response = None\n",
    "        while True:\n",
    "            try:\n",
    "                response = openai.ChatCompletion.create(\n",
    "                    model=\"gpt-3.5-turbo\",\n",
    "                    messages=[\n",
    "                        { \"role\": \"user\", \"content\": this_prompt }\n",
    "                    ],\n",
    "                    temperature=0.7,\n",
    "                    max_tokens=128,\n",
    "                    n=completions_per_headline\n",
    "                )\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(\"retrying...\")\n",
    "                time.sleep(1)\n",
    "\n",
    "        for choice in response[\"choices\"]:\n",
    "            completions[original].append(choice[\"message\"][\"content\"])\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(completions, f, indent=4)\n",
    "\n",
    "    return completions\n",
    "\n",
    "def generate_explanations2(df, filename, completions_per_headline=1):\n",
    "    prompt = \"\"\"\n",
    "        The following news headlines have been edited to be more humorous.\n",
    "        The format of the headline is \"text text [[ original word => edited word ]] text text\".\n",
    "        Explain what kind of humorous response the edit wanted to elicit, and wether it suceeeded or fell flat.\n",
    "        You are not to be too easily offended. Answer as concisely as possible. When explaining something refer to the exact part in the headline.\n",
    "        Do not use more than 3 sentences. Only output the explanation, nothing else.\n",
    "\n",
    "        Headline:\n",
    "        REPLACE_WITH_HEADLINE\n",
    "    \"\"\"\n",
    "\n",
    "    completions = defaultdict(list)\n",
    "    for i in range(len(df.index)):\n",
    "        headline = df.iloc[i]\n",
    "        original = headline[\"original\"]\n",
    "        edit     = headline[\"edit\"]\n",
    "        combined = original.replace(\"<\", \"[[ \").replace(\"/>\", f\" => {edit} ]]\")\n",
    "        \n",
    "        this_prompt = prompt.replace(\"REPLACE_WITH_HEADLINE\", combined)\n",
    "\n",
    "        print(\"generating\", i, \"/\", len(df.index))\n",
    "\n",
    "        request = {\n",
    "            'user_input': this_prompt,\n",
    "            'history': {'internal': [], 'visible': []},\n",
    "            'mode': 'instruct',  # Valid options: 'chat', 'chat-instruct', 'instruct'\n",
    "            'character': 'Example',\n",
    "            'instruction_template': 'Vicuna-v1.1',\n",
    "\n",
    "            'regenerate': False,\n",
    "            '_continue': False,\n",
    "            'stop_at_newline': True,\n",
    "            'chat_prompt_size': 2048,\n",
    "            'chat_generation_attempts': 1,\n",
    "            'chat-instruct_command': 'Continue the chat dialogue below. Write a single reply for the character \"<|character|>\".\\n\\n<|prompt|>',\n",
    "\n",
    "            'max_new_tokens': 250,\n",
    "            'do_sample': True,\n",
    "            'temperature': 0.7,\n",
    "            'top_p': 0.1,\n",
    "            'typical_p': 1,\n",
    "            'repetition_penalty': 1.18,\n",
    "            'top_k': 40,\n",
    "            'min_length': 0,\n",
    "            'no_repeat_ngram_size': 0,\n",
    "            'num_beams': 1,\n",
    "            'penalty_alpha': 0,\n",
    "            'length_penalty': 1,\n",
    "            'early_stopping': True,\n",
    "            'seed': -1,\n",
    "            'add_bos_token': True,\n",
    "            'truncation_length': 2048,\n",
    "            'ban_eos_token': False,\n",
    "            'skip_special_tokens': True,\n",
    "            'stopping_strings': []\n",
    "        }\n",
    "\n",
    "        response = requests.post(\"http://127.0.0.1:5000/api/v1/chat\", json=request)\n",
    "        print(response.status_code)\n",
    "        if response.status_code == 200:\n",
    "            result = response.json()['results'][0]['history']\n",
    "            print(json.dumps(result, indent=4))\n",
    "            print()\n",
    "            print(result['visible'][-1][1])\n",
    "            completions[original].append(result['visible'][-1][1])\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(completions, f, indent=4)\n",
    "\n",
    "    return completions\n",
    "\n",
    "\n",
    "x = generate_explanations2(cached_base_dataset[0], \"explanations_train.json\", 1)\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"completions_1703.json\", \"w\") as f:\n",
    "    json.dump(completions, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Environment",
   "language": "python",
   "name": "tpenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
