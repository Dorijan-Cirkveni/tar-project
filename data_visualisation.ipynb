{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import iqr\n",
    "from datasets import load_dataset\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import *\n",
    "train_df, val_df, test_df = make_base_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_df\n",
    "combined_df = pd.concat([train_df, val_df, test_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Histogram of mean grades\n",
    "sns.set_style(\"ticks\")\n",
    "sns.histplot(combined_df['meanGrade'], palette=\"flare\", bins=16, cumulative=False)\n",
    "plt.xlabel('Mean Grade')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "len(combined_df[combined_df['meanGrade'] <= 0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the grades data\n",
    "flat_data = df.explode('grades')\n",
    "\n",
    "# Convert 'grades' to numeric\n",
    "flat_data['grades'] = pd.to_numeric(flat_data['grades'])\n",
    "\n",
    "# Boxplot of grades\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "fig.set_size_inches(10, 5)\n",
    "\n",
    "flat_data.boxplot(column='meanGrade', ax=ax[0])\n",
    "ax[0].set_title('Boxplot of Mean Grades')\n",
    "ax[0].set_ylabel('Grades')\n",
    "\n",
    "flat_data.boxplot(column='most_common_grade', ax=ax[1])\n",
    "ax[1].set_title('Boxplot of most common grades')\n",
    "ax[1].set_ylabel('Grades')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Scatter plot of mean grades vs standard deviation of grades\n",
    "plt.scatter(df['meanGrade'], df['stddev'])\n",
    "plt.title('Scatter Plot of Mean Grades vs Standard Deviation')\n",
    "plt.xlabel('Mean Grade')\n",
    "plt.ylabel('Standard Deviation')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print 5 funniest lines\n",
    "print(\"\\n5 Funniest Lines:\")\n",
    "funniest = combined_df.sort_values(by='meanGrade', ascending=False).head(50)\n",
    "for i, row in funniest.iterrows():\n",
    "    print(f\"{row['arrow_sentence']}, Mean Grade: {row['meanGrade']}\")\n",
    "\n",
    "# Print 5 least funny lines\n",
    "print(\"\\n5 Least Funny Lines:\")\n",
    "least_funny = combined_df.sort_values(by='meanGrade').head(50)\n",
    "for i, row in least_funny.iterrows():\n",
    "    print(f\"{row['arrow_sentence']}, Mean Grade: {row['meanGrade']}\")\n",
    "\n",
    "# Print 5 lines with biggest variance\n",
    "print(\"\\n10 Lines With Biggest Variance:\")\n",
    "biggest_variance = combined_df.sort_values(by='stddev', ascending=False).head(50)\n",
    "for i, row in biggest_variance.iterrows():\n",
    "    print(f\"{row['arrow_sentence']}, Mean: {row['meanGrade']}, Std Dev: {row['stddev']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Bucketize stddev into 4 bins\n",
    "df['stddev_bucket'], bins = pd.cut(df['stddev'], bins=4, retbins=True, labels=[\"low\", \"medium-low\", \"medium-high\", \"high\"])\n",
    "\n",
    "# Get unique labels\n",
    "labels = df['stddev_bucket'].unique().sort_values()\n",
    "\n",
    "print(bins)\n",
    "print(labels)\n",
    "\n",
    "# 2. Create a histogram of mean grades for each stddev bucket.\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 14))\n",
    "i = 0\n",
    "for i in range(len(labels)):\n",
    "    label = labels[i]\n",
    "    bin_from, bin_to = bins[i], bins[i + 1]\n",
    "    axs[i // 2, i % 2].hist(df[df['stddev_bucket'] == label]['meanGrade'], bins=10, edgecolor='black')\n",
    "    axs[i // 2, i % 2].set_title(f'Histogram of Mean Grades for {label.capitalize()} Stddev Bucket\\n({bin_from:.2f} - {bin_to:.2f})')\n",
    "    axs[i // 2, i % 2].set_xlabel('Mean Grade')\n",
    "    axs[i // 2, i % 2].set_ylabel('Count')\n",
    "    i += 1\n",
    "plt.show()\n",
    "\n",
    "# 3. Plot a histogram of all individual grades for rows that fall into a particular stddev bucket.\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 14))\n",
    "i = 0\n",
    "for i in range(len(labels)):\n",
    "    label = labels[i]\n",
    "    bin_from, bin_to = bins[i], bins[i + 1]\n",
    "    grades_in_bucket = df[df['stddev_bucket'] == label]['grades']\n",
    "    grades_in_bucket = [item for sublist in grades_in_bucket for item in sublist]  # flatten list of lists\n",
    "\n",
    "    axs[i // 2, i % 2].hist(grades_in_bucket, bins=range(5), edgecolor='black', align='left')\n",
    "    axs[i // 2, i % 2].set_title(f'Histogram of Individual Grades for {label.capitalize()} Stddev Bucket\\n({bin_from:.2f} - {bin_to:.2f})')\n",
    "    axs[i // 2, i % 2].set_xlabel('Grade')\n",
    "    axs[i // 2, i % 2].set_ylabel('Count')\n",
    "    axs[i // 2, i % 2].set_xticks(range(4))\n",
    "    i += 1\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "try:\n",
    "    \n",
    "    with open('train_topics_cache.json', 'r') as f:\n",
    "        topics = json.load(f)\n",
    "        df['topics'] = df['id'].apply(lambda id: topics[id])\n",
    "except:\n",
    "\n",
    "    from transformers import pipeline\n",
    "    import torch\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Assume that these are your news headlines\n",
    "    headlines = []\n",
    "    for i, row in df.iterrows():\n",
    "        o, e = row['original'], row['edit']\n",
    "        #h = o.replace('<', '(').replace('/>', f', {e})')\n",
    "        h = o\n",
    "        headlines.append(h)\n",
    "\n",
    "    # Define the topics you want to classify the headlines into\n",
    "    topics = [\"Business\", \"Science\", \"Health\", \"Politics\", \"Democrats\", \"Republicans\", \"Trump\", \"Biden\", \"Economy\", \"Sports\", \"Entertainment\", \"Technology\", \"Education\", \"World\", \"US\", \"Crime\"]\n",
    "\n",
    "    # Create a zero-shot classification pipeline\n",
    "    classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=0)\n",
    "\n",
    "    # Iterate over the headlines and classify each into a topic\n",
    "    df['topics'] = ''\n",
    "\n",
    "    batch_size = 1000\n",
    "    i = 0\n",
    "    while i < len(headlines):\n",
    "        # Make the prediction\n",
    "        result = classifier(headlines[i:i+batch_size], topics)\n",
    "\n",
    "        # Get the best matching topic\n",
    "        for j, r in enumerate(result):\n",
    "            scores = sorted(r['scores'])\n",
    "            best_topic        = r['labels'][r['scores'].index(scores[-1])]\n",
    "            second_best_topic = r['labels'][r['scores'].index(scores[-2])]\n",
    "            third_best_topic  = r['labels'][r['scores'].index(scores[-3])]\n",
    "\n",
    "            #print(f'Headline: {i+j}/{len(headlines)} \"{headlines[i+j]}\" \\nBest Matching Topics: {best_topic}, {second_best_topic}, {third_best_topic}')\n",
    "            df.at[i+j, 'topics'] = [best_topic, second_best_topic, third_best_topic]\n",
    "\n",
    "        i += batch_size\n",
    "        print(f'Finished headlines {i}/{len(headlines)}')\n",
    "\n",
    "    topics = {}\n",
    "    for i, row in df.iterrows():\n",
    "        topics[row['id']] = row['topics']\n",
    "\n",
    "    with open('train_topics_cache.txt', 'w') as f:\n",
    "        json.dump(topics, f)\n",
    "\n",
    "\n",
    "try:\n",
    "    with open('train_edit_topics_cache.json', 'r') as f:\n",
    "        topics = json.load(f)\n",
    "        df['edit_topics'] = df['id'].apply(lambda id: topics[id])\n",
    "except:\n",
    "\n",
    "    from transformers import pipeline\n",
    "    import torch\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Assume that these are your news headlines\n",
    "    headlines = []\n",
    "    for i, row in df.iterrows():\n",
    "        o, e = row['original'], row['edit']\n",
    "        #h = o.replace('<', '(').replace('/>', f', {e})')\n",
    "        #h = o\n",
    "        h = e\n",
    "        headlines.append(h)\n",
    "\n",
    "    # Define the topics you want to classify the funny words\n",
    "    topics = [\"sexual\", \"bodypart\", \"family\", \"food\", \"trump\", \"politics\", \"children\", \"clothes\", \"fashion\", \"animals\" , \"drugs\", \"silly\", \"places\", \"profession\", \"disease\", \"relationships\", \"aging\", \"money\" ]\n",
    "\n",
    "    # Create a zero-shot classification pipeline\n",
    "    classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=0)\n",
    "\n",
    "    # Iterate over the headlines and classify each into a topic\n",
    "    df['edit_topics'] = ''\n",
    "\n",
    "    batch_size = 100\n",
    "    i = 0\n",
    "    while i < len(headlines):\n",
    "        # Make the prediction\n",
    "        result = classifier(headlines[i:i+batch_size], topics)\n",
    "\n",
    "        # Get the best matching topic\n",
    "        for j, r in enumerate(result):\n",
    "            scores = sorted(r['scores'])\n",
    "            best_topic        = r['labels'][r['scores'].index(scores[-1])]\n",
    "            second_best_topic = r['labels'][r['scores'].index(scores[-2])]\n",
    "            third_best_topic  = r['labels'][r['scores'].index(scores[-3])]\n",
    "\n",
    "            print(f'Headline: {i+j}/{len(headlines)} \"{headlines[i+j]}\", Best Matching Topics: {best_topic}, {second_best_topic}, {third_best_topic}')\n",
    "            df.at[i+j, 'edit_topics'] = [best_topic, second_best_topic, third_best_topic]\n",
    "\n",
    "        i += batch_size\n",
    "        print(f'Finished headlines {i}/{len(headlines)}')\n",
    "\n",
    "    topics = {}\n",
    "    for i, row in df.iterrows():\n",
    "        topics[row['id']] = row['edit_topics']\n",
    "\n",
    "    with open('train_edit_topics_cache.json', 'w') as f:\n",
    "        json.dump(topics, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode the dataframe on 'topics' so each topic is in a separate row\n",
    "#df_exploded = df.explode('topics')\n",
    "df_exploded = df.copy()\n",
    "df_exploded['topics'] = df['topics'].apply(lambda ts: ts[0])\n",
    "\n",
    "# Box plot of grades per topic\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.boxplot(x=\"topics\", y=\"meanGrade\", data=df_exploded)\n",
    "plt.title('Box Plot of Mean Grades per Topic')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot of mean grades vs stddev, colored by topic\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(x=\"meanGrade\", y=\"stddev\", hue=\"topics\", data=df_exploded)\n",
    "plt.title('Scatter Plot of Mean Grades vs Standard Deviation, Colored by Topic')\n",
    "plt.show()\n",
    "\n",
    "# Two additional visualizations:\n",
    "# 1. Count plot of topics\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.countplot(x=\"topics\", data=df_exploded, order = df_exploded['topics'].value_counts().index)\n",
    "plt.title('Count of Headlines per Topic')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "# 2. Box plot of stddev per topic\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.boxplot(x=\"topics\", y=\"stddev\", data=df_exploded)\n",
    "plt.title('Box Plot of Stddev per Topic')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df.index))\n",
    "\n",
    "# Explode the dataframe on 'topics' so each topic is in a separate row\n",
    "#df_exploded = df.explode('topics')\n",
    "df_edit_topics = df.copy()\n",
    "df_edit_topics['edit_topics'] = df['edit_topics'].apply(lambda ts: ts[0])\n",
    "\n",
    "# Box plot of grades per topic\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.boxplot(x=\"edit_topics\", y=\"meanGrade\", data=df_edit_topics)\n",
    "plt.title('Box Plot of Mean Grades per Edit Topic')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot of mean grades vs stddev, colored by topic\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(x=\"meanGrade\", y=\"stddev\", hue=\"edit_topics\", data=df_edit_topics)\n",
    "plt.title('Scatter Plot of Mean Grades vs Standard Deviation, Colored by Edit Topic')\n",
    "plt.show()\n",
    "\n",
    "# Two additional visualizations:\n",
    "# 1. Count plot of edit_topics\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.countplot(x=\"edit_topics\", data=df_edit_topics, order = df_edit_topics['edit_topics'].value_counts().index)\n",
    "plt.title('Count of Headlines per Edit Topic')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "# 2. Box plot of stddev per topic\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.boxplot(x=\"edit_topics\", y=\"stddev\", data=df_edit_topics)\n",
    "plt.title('Box Plot of Stddev per Edit Topic')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agreement_names = ['All Agree', 'One Disagreement', 'Two Disagreements', 'Three Disagreements']\n",
    "\n",
    "def deconstruct_by_agreeableness(df):\n",
    "    df_all_agree = df[df['disagreements'] == 0]\n",
    "    df_one_disagree = df[df['disagreements'] == 1]\n",
    "    df_two_disagree = df[df['disagreements'] == 2]\n",
    "    df_three_disagree = df[df['disagreements'] == 3]\n",
    "    assert len(df_all_agree.index) + len(df_one_disagree.index) + len(df_two_disagree.index) + len(df_three_disagree.index) == len(df.index)\n",
    "    return df_all_agree, df_one_disagree, df_two_disagree, df_three_disagree\n",
    "\n",
    "df_all_agree, df_one_disagree, df_two_disagree, df_three_disagree = deconstruct_by_agreeableness(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "dfs = deconstruct_by_agreeableness(df)\n",
    "for i, df_agreement in enumerate(dfs):\n",
    "    print(i)\n",
    "    # remove all where topic is trump\n",
    "    df_agreement['topics'] = df['topics'].apply(lambda ts: ts[0])\n",
    "    df_agreement = df_agreement[df_agreement['topics'] != 'trump']\n",
    "\n",
    "    axs[i // 2, i % 2].hist(df_agreement['most_common_grade'], bins=4, edgecolor='black')\n",
    "    axs[i // 2, i % 2].set_title(f\"Most common grade for: {agreement_names[i]}\")\n",
    "    axs[i // 2, i % 2].set_xlabel('Mean Grade')\n",
    "    axs[i // 2, i % 2].set_ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode the dataframe on 'topics' so each topic is in a separate row\n",
    "#df_exploded = df.explode('topics')\n",
    "df_exploded = df_one_disagree.copy()\n",
    "df_exploded['topics'] = df['topics'].apply(lambda ts: ts[0])\n",
    "\n",
    "# for each topic, get the mean most common grade and stddev for most common grade, and set as new cols\n",
    "df_exploded['most_common_grade'] = df_exploded['most_common_grade'].astype(float)\n",
    "df_exploded['mean_most_common_grade'] = df_exploded.groupby('topics')['most_common_grade'].transform('mean')\n",
    "df_exploded['stddev_most_common_grade'] = df_exploded.groupby('topics')['most_common_grade'].transform('std')\n",
    "\n",
    "# Box plot of grades per topic\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.boxplot(x=\"topics\", y=\"meanGrade\", data=df_exploded)\n",
    "plt.title('Box Plot of Mean Grades per Topic')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot of mean grades vs stddev, colored by topic\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(x=\"meanGrade\", y=\"stddev\", hue=\"topics\", data=df_exploded)\n",
    "plt.title('Scatter Plot of Mean Grades vs Standard Deviation, Colored by Topic')\n",
    "plt.show()\n",
    "\n",
    "# Two additional visualizations:\n",
    "# 1. Count plot of topics\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.countplot(x=\"topics\", data=df_exploded, order = df_exploded['topics'].value_counts().index)\n",
    "plt.title('Count of Headlines per Topic')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "# 2. Box plot of stddev per topic\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.boxplot(x=\"topics\", y=\"stddev\", data=df_exploded)\n",
    "plt.title('Box Plot of Stddev per Topic')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from model_test import fit_regression, compute_metrics_regression\n",
    "from data import TokenizedSentencesDataset\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "model, trainer = fit_regression('sentence-transformers/all-MiniLM-L6-v2', './fit_out', train_df, val_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizedSentencesDataset(Dataset):\n",
    "    def __init__(self, tokenizer, dataframe):\n",
    "        self.df = dataframe\n",
    "        \n",
    "        original = self.df[\"original_sentence\"].tolist()\n",
    "        edited   = self.df[\"edited_sentence\"].tolist()\n",
    "        text = [f\"{o} [SEP] {e}\" for o, e in zip(original, edited)]\n",
    "        print(len(text))\n",
    "        output = tokenizer(text=text, truncation=True, padding=True, return_tensors='pt')\n",
    "        \n",
    "        self.input_ids      = output[\"input_ids\"]\n",
    "        self.attention_mask = output[\"attention_mask\"]\n",
    "        #self.labels         = torch.tensor(self.df['most_common_grade'].values, dtype=torch.float32)\n",
    "        self.labels         = torch.tensor(self.df['normalized_score'].values, dtype=torch.float32)\n",
    "    def __len__(self):\n",
    "        return len(self.df.index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\":      self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_mask[idx],\n",
    "            \"labels\":         self.labels[idx],\n",
    "        }\n",
    "\n",
    "# put computed metrics into new dataframe\n",
    "df_metrics = pd.DataFrame(columns=['agreement', 'grade', 'pearson', 'spearman', 'rmse'])\n",
    "overall_mses = []\n",
    "for i, df_agreement in enumerate(deconstruct_by_agreeableness(test_df)):\n",
    "    for grade in range(0, 3+1):\n",
    "        df_agrement_and_grade = df_agreement[df_agreement['most_common_grade'] == grade]\n",
    "        if len(df_agrement_and_grade) == 0:\n",
    "            continue\n",
    "        out = trainer.predict(TokenizedSentencesDataset(tokenizer, df_agrement_and_grade))\n",
    "        metrics = compute_metrics_regression((out.predictions, out.label_ids))\n",
    "        print(f\"Metrics for {agreement_names[i]}, grade {grade}: {metrics}\")\n",
    "\n",
    "        # store metric in dataframe\n",
    "        df_metrics = df_metrics.append({\n",
    "            'agreement': agreement_names[i],\n",
    "            'grade': grade,\n",
    "            #'pearson': metrics['pearson'],\n",
    "            #'spearman': metrics['spearman'],\n",
    "            'rmse': metrics['rmse'],\n",
    "        }, ignore_index=True)\n",
    "    \n",
    "    out = trainer.predict(TokenizedSentencesDataset(tokenizer, df_agreement))\n",
    "    metrics = compute_metrics_regression((out.predictions, out.label_ids))\n",
    "    df_metrics = df_metrics.append({\n",
    "            'agreement': agreement_names[i],\n",
    "            'grade': -1,\n",
    "            #'pearson': metrics['pearson'],\n",
    "            #'spearman': metrics['spearman'],\n",
    "            'rmse': metrics['rmse'],\n",
    "        }, ignore_index=True)\n",
    "    overall_mses.append(metrics['rmse'])\n",
    "\n",
    "    print(f\"Metrics for {agreement_names[i]}, combined grades: {metrics}\")\n",
    "\n",
    "# separate barplot of rmse for each agreement level\n",
    "plt.figure(figsize=(12, 8))\n",
    "ax = sns.barplot(x=\"agreement\", y=\"rmse\", hue=\"grade\", data=df_metrics[df_metrics['grade'] != -1], palette=\"flare\")\n",
    "ax.hlines(\n",
    "    y=overall_mses, \n",
    "    xmin=[i-0.5 for i in range(len(overall_mses))],\n",
    "    xmax=[i+0.5 for i in range(len(overall_mses))], \n",
    "    ls='--', color='black'\n",
    ")\n",
    "plt.title('RMSE for Each Agreement Level')\n",
    "#plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "ax = sns.barplot(x=\"agreement\", y=\"rmse\", data=df_metrics[df_metrics['grade'] == -1], palette=\"flare\")\n",
    "ax.axhline(0.55, ls='--', color='black')\n",
    "plt.title('Overall RMSE')\n",
    "#plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "print(df_metrics[(df_metrics['grade'] == 3) & (df_metrics['agreement'] == 'All Agree')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_metrics[(df_metrics['grade'] == 3) & (df_metrics['agreement'] == 'All Agree')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = deconstruct_by_agreeableness(df)\n",
    "\n",
    "# Compute grade counts for each dataframe\n",
    "grade_counts = [df['most_common_grade'].value_counts(normalize=False) for df in dfs]\n",
    "\n",
    "# Convert grade counts to a DataFrame\n",
    "grade_counts_df = pd.DataFrame(grade_counts).T\n",
    "grade_counts_df.columns = agreement_names\n",
    "\n",
    "# Plot\n",
    "grade_counts_df.plot(kind='bar', stacked=True, figsize=(10, 7), color=sns.color_palette(\"flare\", n_colors=4)[::-1])\n",
    "plt.title('Agreement Classes and Most Common Grades')\n",
    "plt.xlabel('N')\n",
    "plt.ylabel('Grade')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "dfs = deconstruct_by_agreeableness(combined_df)\n",
    "\n",
    "# Create bins for meanGrade\n",
    "bins = np.linspace(0, 3, 17)  # 16 bins between 0 and 3\n",
    "\n",
    "# Create a new dataframe list for storing modified dfs\n",
    "dfs_mod = []\n",
    "\n",
    "# Create a new column in each dataframe for the binned meanGrade\n",
    "for df in dfs:\n",
    "    df_copy = df.copy()  # Copy the DataFrame to avoid SettingWithCopyWarning\n",
    "    df_copy['meanGrade_bin'] = pd.cut(df_copy['meanGrade'], bins, labels=False, include_lowest=True) + 1\n",
    "    dfs_mod.append(df_copy)\n",
    "\n",
    "# Compute grade counts for each dataframe\n",
    "grade_counts = [df['mean_grade_bin'].value_counts(normalize=False).sort_index() for df in dfs_mod]\n",
    "\n",
    "# Convert grade counts to a DataFrame\n",
    "grade_counts_df = pd.DataFrame(grade_counts).T\n",
    "grade_counts_df.columns = [\"5/5 Agree (Consensus)\", \"4/5 Agree\", \"3/5 Agree\", \"2/5 Agree\"]\n",
    "\n",
    "# Plot\n",
    "#diverging_colors = sns.color_palette(\"RdPu\", 10)\n",
    "#\n",
    "grade_counts_df.plot(kind='bar', stacked=True, figsize=(10, 7), width=1.0, edgecolor='black', color=sns.color_palette(\"Blues\", n_colors=4)[::-1])\n",
    "plt.xlabel('Mean Grade Range')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(ticks=range(16), labels=[\"{0:.1f}-{1:.1f}\".format(bins[i], bins[i+1]) for i in range(16)], rotation=45)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute disagreement counts for each dataframe\n",
    "disagreement_counts = [df['most_common_grade'].value_counts(normalize=True) for df in dfs]\n",
    "\n",
    "# Convert disagreement counts to a DataFrame\n",
    "disagreement_counts_df = pd.DataFrame(disagreement_counts).T\n",
    "disagreement_counts_df.columns = ['5/5 Agree\\n(Concensus)', '4/5 Agree', '3/5 Agree', '2/5 Agree']\n",
    "\n",
    "# Transpose the DataFrame to get disagreement classes as bars\n",
    "disagreement_counts_df = disagreement_counts_df.transpose()\n",
    "\n",
    "# Plot\n",
    "cmap = sns.color_palette(\"flare\", n_colors=4)[::-1]\n",
    "\n",
    "disagreement_counts_df.plot(kind='barh', stacked=True, figsize=(10, 7), color=cmap).invert_yaxis()\n",
    "plt.title('Grade Distribution per Agreement Class')\n",
    "plt.xlabel('Percentage')\n",
    "plt.ylabel('Agreement Class')\n",
    "plt.legend(title='Most Common Grade')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "#{ 'dataset': 'KEPT'   model: 'randomly sampled',  disagreement 0 (threshold 0.8): {'mae': 0.7708974409103394, 'mse': 0.7549566245722571, 'rmse': 0.8688823997367291}\n",
    "#{ 'dataset': 'KEPT'   model: 'refined data',      disagreement 0 (threshold 0.8): {'mae': 1.1947281217575074, 'mse': 1.6457225016459205, 'rmse': 1.2828571633841082}\n",
    "#{ 'dataset': 'ENTIRE' model: 'randomly sampled',  disagreement 0 (threshold 0.8): {'mae': 0.8496183668865877, 'mse': 0.8022210720854362, 'rmse': 0.8956679474478454}\n",
    "#{ 'dataset': 'ENTIRE' model: 'refined data',      disagreement 0 (threshold 0.8): {'mae': 1.3842397381277645, 'mse': 2.0110816431274294, 'rmse': 1.4181261026888368}\n",
    "#{ 'dataset': 'KEPT'   model: 'randomly sampled',  disagreement 1 (threshold 0.8): {'mae': 0.5950841200351715, 'mse': 0.4587049847721751, 'rmse': 0.677277627544403}\n",
    "#{ 'dataset': 'KEPT'   model: 'refined data',      disagreement 1 (threshold 0.8): {'mae': 0.9578437507152557, 'mse': 1.104062448594309, 'rmse': 1.0507437597218023}\n",
    "#{ 'dataset': 'ENTIRE' model: 'randomly sampled',  disagreement 1 (threshold 0.8): {'mae': 0.6172205818002112, 'mse': 0.45669913798666817, 'rmse': 0.6757951893781637}\n",
    "#{ 'dataset': 'ENTIRE' model: 'refined data',      disagreement 1 (threshold 0.8): {'mae': 1.0630526703898795, 'mse': 1.2588998793999693, 'rmse': 1.122007076359133}\n",
    "#{ 'dataset': 'KEPT'   model: 'randomly sampled',  disagreement 2 (threshold 0.8): {'mae': 0.5152952637684558, 'mse': 0.4047397753387257, 'rmse': 0.636191618412822}\n",
    "#{ 'dataset': 'KEPT'   model: 'refined data',      disagreement 2 (threshold 0.8): {'mae': 0.5900502785774892, 'mse': 0.48183372218797293, 'rmse': 0.6941424365272396}\n",
    "#{ 'dataset': 'ENTIRE' model: 'randomly sampled',  disagreement 2 (threshold 0.8): {'mae': 0.4306439177331248, 'mse': 0.2720599061841849, 'rmse': 0.5215936216866393}\n",
    "#{ 'dataset': 'ENTIRE' model: 'refined data',      disagreement 2 (threshold 0.8): {'mae': 0.7028733493840684, 'mse': 0.6203621750667347, 'rmse': 0.7876307352222454}\n",
    "#{ 'dataset': 'KEPT'   model: 'randomly sampled',  disagreement 3 (threshold 0.8): {'mae': 0.4303677698520765, 'mse': 0.2950672052133709, 'rmse': 0.5432008884504617}\n",
    "#{ 'dataset': 'KEPT'   model: 'refined data',      disagreement 3 (threshold 0.8): {'mae': 0.3773945832633537, 'mse': 0.20071710776042836, 'rmse': 0.4480146289580602}\n",
    "#{ 'dataset': 'ENTIRE' model: 'randomly sampled',  disagreement 3 (threshold 0.8): {'mae': 0.38164039871617816, 'mse': 0.2424444785399681, 'rmse': 0.4923865133611684}\n",
    "#{ 'dataset': 'ENTIRE' model: 'refined data',      disagreement 3 (threshold 0.8): {'mae': 0.4021579368232077, 'mse': 0.22083215462270328, 'rmse': 0.46992781852397636}\n",
    "\n",
    "#KEPT   dataset, entire set model, disagreement 0 (threshold 0.8): {'mae': 0.7413814520835876, 'mse': 0.7024504329416097, 'rmse': 0.8381231609624028}\n",
    "#KEPT   dataset, kept-only        model, disagreement 0 (threshold 0.8): {'mae': 1.2192668390274048, 'mse': 1.7215445900930462, 'rmse': 1.3120764421683084}\n",
    "#ENTIRE dataset, entire set model, disagreement 0 (threshold 0.8): {'mae': 0.7974914252758026, 'mse': 0.7029073651703219, 'rmse': 0.8383957091793361}\n",
    "#ENTIRE dataset, kept-only        model, disagreement 0 (threshold 0.8): {'mae': 1.3947345298879288, 'mse': 2.046829373529656, 'rmse': 1.4306744470806962}\n",
    "#KEPT   dataset, entire set model, disagreement 1 (threshold 0.8): {'mae': 0.586527540922165, 'mse': 0.446907995741235, 'rmse': 0.6685117768156632}\n",
    "#KEPT   dataset, kept-only        model, disagreement 1 (threshold 0.8): {'mae': 0.9625296218395233, 'mse': 1.1041413820081165, 'rmse': 1.050781319784529}\n",
    "#ENTIRE dataset, entire set model, disagreement 1 (threshold 0.8): {'mae': 0.5768714070436545, 'mse': 0.41018517171680546, 'rmse': 0.6404570022388744}\n",
    "#ENTIRE dataset, kept-only        model, disagreement 1 (threshold 0.8): {'mae': 1.0701831144397147, 'mse': 1.2728756160988088, 'rmse': 1.128217893892314}\n",
    "#KEPT   dataset, entire set model, disagreement 2 (threshold 0.8): {'mae': 0.5172069427621273, 'mse': 0.40481194649308855, 'rmse': 0.6362483371240263}\n",
    "#KEPT   dataset, kept-only        model, disagreement 2 (threshold 0.8): {'mae': 0.58880672397201, 'mse': 0.4808566933091022, 'rmse': 0.6934383125477724}\n",
    "#ENTIRE dataset, entire set model, disagreement 2 (threshold 0.8): {'mae': 0.4077129732960108, 'mse': 0.2566611542477327, 'rmse': 0.5066173647317399}\n",
    "#ENTIRE dataset, kept-only        model, disagreement 2 (threshold 0.8): {'mae': 0.706048510962613, 'mse': 0.6227710308918454, 'rmse': 0.7891584320602837}\n",
    "#KEPT   dataset, entire set model, disagreement 3 (threshold 0.8): {'mae': 0.4178176676029484, 'mse': 0.27900516632344835, 'rmse': 0.5282093962847011}\n",
    "#KEPT   dataset, kept-only        model, disagreement 3 (threshold 0.8): {'mae': 0.3797878618926218, 'mse': 0.20275371804176529, 'rmse': 0.4502818206876281}\n",
    "#ENTIRE dataset, entire set model, disagreement 3 (threshold 0.8): {'mae': 0.3868751161836188, 'mse': 0.24948764664997905, 'rmse': 0.4994873838746871}\n",
    "#ENTIRE dataset, kept-only        model, disagreement 3 (threshold 0.8): {'mae': 0.40747104749551266, 'mse': 0.22633906634991705, 'rmse': 0.47575105501713505}\n",
    "#KEPT   dataset, entire set model, meanGrade 0.0-0.5 (threshold 0.8): {'mae': 0.7045136007997724, 'mse': 0.5272123278862542, 'rmse': 0.7260938836584799}\n",
    "\n",
    "{ 'dataset': 'KEPT',   'model': 'kept-only',        'mean_grade_bucket': '0.0 - 0.5', 'rmse': 1.2640821777011984 },\n",
    "{ 'dataset': 'ENTIRE', 'model': 'kept-only',        'mean_grade_bucket': '0.0 - 0.5', 'rmse': 1.2658762338133176 },\n",
    "{ 'dataset': 'KEPT',   'model': 'kept-only',        'mean_grade_bucket': '0.5 - 1.0', 'rmse': 0.7409811463413085 },\n",
    "{ 'dataset': 'ENTIRE', 'model': 'kept-only',        'mean_grade_bucket': '0.5 - 1.0', 'rmse': 0.7535737631897971 },\n",
    "{ 'dataset': 'KEPT',   'model': 'kept-only',        'mean_grade_bucket': '1.0 - 1.5', 'rmse': 0.3892582724031099 },\n",
    "{ 'dataset': 'ENTIRE', 'model': 'kept-only',        'mean_grade_bucket': '1.0 - 1.5', 'rmse': 0.3995633921945709 },\n",
    "{ 'dataset': 'KEPT',   'model': 'kept-only',        'mean_grade_bucket': '1.5 - 2.0', 'rmse': 0.27838517310414024 },\n",
    "{ 'dataset': 'ENTIRE', 'model': 'kept-only',        'mean_grade_bucket': '1.5 - 2.0', 'rmse': 0.27338923178002955 },\n",
    "{ 'dataset': 'KEPT',   'model': 'kept-only',        'mean_grade_bucket': '2.0 - 2.5', 'rmse': 0.609835113508833 },\n",
    "{ 'dataset': 'ENTIRE', 'model': 'kept-only',        'mean_grade_bucket': '2.0 - 2.5', 'rmse': 0.594459892897418 },\n",
    "{ 'dataset': 'KEPT',   'model': 'kept-only',        'mean_grade_bucket': '2.5 - 3.0', 'rmse': 1.0530526028551583 },\n",
    "{ 'dataset': 'ENTIRE', 'model': 'kept-only',        'mean_grade_bucket': '2.5 - 3.0', 'rmse': 1.091694462311364 },\n",
    "\n",
    "\n",
    "dinky_df = pd.DataFrame([\n",
    "    { 'dataset': 'KEPT',    'model': 'randomly sampled', 'mean_grade_bucket': '0.0 - 3.0', 'rmse': 0.60956878995761 },\n",
    "    { 'dataset': 'KEPT',    'model': 'refined data',     'mean_grade_bucket': '0.0 - 3.0', 'rmse': 0.682103854550248 },\n",
    "    { 'dataset': 'ENTIRE',  'model': 'randomly sampled', 'mean_grade_bucket': '0.0 - 3.0', 'rmse': 0.5676318313297427 },\n",
    "    { 'dataset': 'ENTIRE',  'model': 'refined data',     'mean_grade_bucket': '0.0 - 3.0', 'rmse': 0.8119993111375877 },\n",
    "    { 'dataset': 'KEPT',    'model': 'randomly sampled', 'mean_grade_bucket': '0.0 - 0.5', 'rmse': 0.7282226979173854 },\n",
    "    { 'dataset': 'KEPT',    'model': 'refined data',     'mean_grade_bucket': '0.0 - 0.5', 'rmse': 1.2630974437920222 },\n",
    "    { 'dataset': 'ENTIRE',  'model': 'randomly sampled', 'mean_grade_bucket': '0.0 - 0.5', 'rmse': 0.7163780496513894 },\n",
    "    { 'dataset': 'ENTIRE',  'model': 'refined data',     'mean_grade_bucket': '0.0 - 0.5', 'rmse': 1.2608005140785197 },\n",
    "    { 'dataset': 'KEPT',    'model': 'randomly sampled', 'mean_grade_bucket': '0.5 - 1.0', 'rmse': 0.25779537198923363 },\n",
    "    { 'dataset': 'KEPT',    'model': 'refined data',     'mean_grade_bucket': '0.5 - 1.0', 'rmse': 0.7298437438630255 },\n",
    "    { 'dataset': 'ENTIRE',  'model': 'randomly sampled', 'mean_grade_bucket': '0.5 - 1.0', 'rmse': 0.2681923719980645 },\n",
    "    { 'dataset': 'ENTIRE',  'model': 'refined data',     'mean_grade_bucket': '0.5 - 1.0', 'rmse': 0.7448765000354526 },\n",
    "    { 'dataset': 'KEPT',    'model': 'randomly sampled', 'mean_grade_bucket': '1.0 - 1.5', 'rmse': 0.2710464052438125 },\n",
    "    { 'dataset': 'KEPT',    'model': 'refined data',     'mean_grade_bucket': '1.0 - 1.5', 'rmse': 0.38479189205515074 },\n",
    "    { 'dataset': 'ENTIRE',  'model': 'randomly sampled', 'mean_grade_bucket': '1.0 - 1.5', 'rmse': 0.27989636911271265 },\n",
    "    { 'dataset': 'ENTIRE',  'model': 'refined data',     'mean_grade_bucket': '1.0 - 1.5', 'rmse': 0.3898444133812833 },\n",
    "    { 'dataset': 'KEPT',    'model': 'randomly sampled', 'mean_grade_bucket': '1.5 - 2.0', 'rmse': 0.7654630273267387 },\n",
    "    { 'dataset': 'KEPT',    'model': 'refined data',     'mean_grade_bucket': '1.5 - 2.0', 'rmse': 0.28044292075600924 },\n",
    "    { 'dataset': 'ENTIRE',  'model': 'randomly sampled', 'mean_grade_bucket': '1.5 - 2.0', 'rmse': 0.770540437243583 },\n",
    "    { 'dataset': 'ENTIRE',  'model': 'refined data',     'mean_grade_bucket': '1.5 - 2.0', 'rmse': 0.28219939850449705 },\n",
    "    { 'dataset': 'KEPT',    'model': 'randomly sampled', 'mean_grade_bucket': '2.0 - 2.5', 'rmse': 1.1279920300625925 },\n",
    "    { 'dataset': 'KEPT',    'model': 'refined data',     'mean_grade_bucket': '2.0 - 2.5', 'rmse': 0.6287969027249929 },\n",
    "    { 'dataset': 'ENTIRE',  'model': 'randomly sampled', 'mean_grade_bucket': '2.0 - 2.5', 'rmse': 1.119837179152282 },\n",
    "    { 'dataset': 'ENTIRE',  'model': 'refined data',     'mean_grade_bucket': '2.0 - 2.5', 'rmse': 0.6114348602316309 },\n",
    "    { 'dataset': 'KEPT',    'model': 'randomly sampled', 'mean_grade_bucket': '2.5 - 3.0', 'rmse': 1.5829224580131365 },\n",
    "    { 'dataset': 'KEPT',    'model': 'refined data',     'mean_grade_bucket': '2.5 - 3.0', 'rmse': 1.052275605984535 },\n",
    "    { 'dataset': 'ENTIRE',  'model': 'randomly sampled', 'mean_grade_bucket': '2.5 - 3.0', 'rmse': 1.6270095279643135 },\n",
    "    { 'dataset': 'ENTIRE',  'model': 'refined data',     'mean_grade_bucket': '2.5 - 3.0', 'rmse': 1.0933530581020658 },\n",
    "    { 'dataset': 'KEPT',    'model': 'entire set',       'mean_grade_bucket': '0.0 - 3.0', 'rmse': 0.6013238178882069 },\n",
    "    { 'dataset': 'ENTIRE',  'model': 'entire set',       'mean_grade_bucket': '0.0 - 3.0', 'rmse': 0.5523551715028504 },\n",
    "    { 'dataset': 'ENTIRE',  'model': 'entire set',       'mean_grade_bucket': '0.0 - 0.5', 'rmse': 0.6673708202475422 },\n",
    "    { 'dataset': 'KEPT',    'model': 'entire set',       'mean_grade_bucket': '0.5 - 1.0', 'rmse': 0.2661807776188036 },\n",
    "    { 'dataset': 'ENTIRE',  'model': 'entire set',       'mean_grade_bucket': '0.5 - 1.0', 'rmse': 0.24179993696017035 },\n",
    "    { 'dataset': 'KEPT',    'model': 'entire set',       'mean_grade_bucket': '1.0 - 1.5', 'rmse': 0.26665174124692165 },\n",
    "    { 'dataset': 'ENTIRE',  'model': 'entire set',       'mean_grade_bucket': '1.0 - 1.5', 'rmse': 0.3001270171622522 },\n",
    "    { 'dataset': 'KEPT',    'model': 'entire set',       'mean_grade_bucket': '1.5 - 2.0', 'rmse': 0.745534987183211 },\n",
    "    { 'dataset': 'ENTIRE',  'model': 'entire set',       'mean_grade_bucket': '1.5 - 2.0', 'rmse': 0.7778305909162193 },\n",
    "    { 'dataset': 'KEPT',    'model': 'entire set',       'mean_grade_bucket': '2.0 - 2.5', 'rmse': 1.0956012691388783 },\n",
    "    { 'dataset': 'ENTIRE',  'model': 'entire set',       'mean_grade_bucket': '2.0 - 2.5', 'rmse': 1.119018057863962 },\n",
    "    { 'dataset': 'KEPT',    'model': 'entire set',       'mean_grade_bucket': '2.5 - 3.0', 'rmse': 1.6075678009840064 },\n",
    "    { 'dataset': 'ENTIRE',  'model': 'entire set',       'mean_grade_bucket': '2.5 - 3.0', 'rmse': 1.6434596593670368 },\n",
    "])\n",
    "\n",
    "sns.barplot(x='mean_grade_bucket', y='rmse', hue='model', palette='flare', data=dinky_df[dinky_df['dataset'] == 'KEPT'])\n",
    "plt.title(\"refined dataset\")\n",
    "plt.show()\n",
    "sns.barplot(x='mean_grade_bucket', y='rmse', hue='model', palette='flare', data=dinky_df[dinky_df['dataset'] == 'ENTIRE'])\n",
    "plt.title(\"entire dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_df = pd.read_json(\"results_grade_buckets.json\")\n",
    "\n",
    "sns.barplot(x='grade_bucket', y='rmse', hue='model', palette='flare', data=loaded_df)\n",
    "plt.title(\"entire dataset\")\n",
    "\n",
    "# calculate average rmse for each model\n",
    "avg_rmse = loaded_df.groupby(['model']).mean().reset_index()\n",
    "print(avg_rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "\n",
    "def round_to(n, fraction):\n",
    "    return round(n / fraction) * fraction\n",
    "\n",
    "\n",
    "loaded_df = pd.read_pickle(\"test_df_normalization_scores.pkl\")\n",
    "\n",
    "#loaded_df['hehe1'] = np.random.normal(0.23448527, 10 * 0.020161796, len(loaded_df))\n",
    "#loaded_df['hehe2'] = np.random.normal(0.49663526, 10 * 0.00527176, len(loaded_df))\n",
    "\n",
    "regression_cols = [\n",
    "    'baseline_regresion_score',\n",
    "    'refined_regression_score',\n",
    "    'mixed_regression_score_scoring',\n",
    "    'mixed_regression_score_lerp',\n",
    "    #'hehe1',\n",
    "    #'hehe2',\n",
    "]\n",
    "\n",
    "for col in regression_cols:\n",
    "    loaded_df[col + '_rmse'] = loaded_df.apply(lambda row: np.sqrt((row[col] - row[\"normalized_score\"])**2) , axis=1)\n",
    "\n",
    "# create column mean_grade_bucket, rounded to nearest 0.1\n",
    "loaded_df['mean_grade_bucket'] = loaded_df['meanGrade'].apply(lambda x: round_to(x, 0.2))\n",
    "\n",
    "# print number of rows in each mean grade bucket\n",
    "print(loaded_df.groupby(['mean_grade_bucket']).count()['id'])\n",
    "print(loaded_df.groupby(['mean_grade_bucket']).count()['id'].sum(), len(loaded_df))\n",
    "\n",
    "print(loaded_df[['id', 'meanGrade', 'mean_grade_bucket']].head(10))\n",
    "# transform so that columns baseline_regresion_score, refined_regression_score, mixed_regression_score_scoring, mixed_regression_score_lerp\n",
    "# become values of a new column \"regression_type\" and their values become values of column rmse\n",
    "def draw1():\n",
    "    dff = loaded_df.melt(\n",
    "        id_vars=['id', 'meanGrade', 'mean_grade_bucket'], \n",
    "        value_vars=[\n",
    "            'baseline_regresion_score_rmse', \n",
    "            'refined_regression_score_rmse', \n",
    "            'mixed_regression_score_scoring_rmse', \n",
    "            'mixed_regression_score_lerp_rmse',\n",
    "            #'hehe1_rmse',\n",
    "            #'hehe2_rmse'\n",
    "        ],\n",
    "        var_name='regression_type', \n",
    "        value_name='rmse'\n",
    "    )\n",
    "\n",
    "    # remove mixed_regression_score_scoring_rmse, mixed_regression_score_lerp_rmse\n",
    "    dff = dff[dff['regression_type'] != 'mixed_regression_score_scoring_rmse']\n",
    "    dff = dff[dff['regression_type'] != 'mixed_regression_score_lerp_rmse']\n",
    "\n",
    "    # compute pearson correlation coefficient\n",
    "    print(\"PEARSON\", dff.groupby(['regression_type']).corr()['rmse'])\n",
    "\n",
    "    sns.lineplot(x='mean_grade_bucket', y='rmse', hue='regression_type', palette='flare', data=dff, style='regression_type', errorbar='sd', markers=True, dashes=False)\n",
    "    plt.title(\"entire dataset\")\n",
    "    plt.show()\n",
    "\n",
    "def draw2():\n",
    "    dff = loaded_df.melt(\n",
    "        id_vars=['id', 'meanGrade', 'mean_grade_bucket'], \n",
    "        value_vars=regression_cols,\n",
    "        var_name='regression_type', \n",
    "        value_name='reg'\n",
    "    )\n",
    "    dff['reg'] *= 3\n",
    "\n",
    "    ### t-test hehe1 vs baseline_regresion_score\n",
    "    ##print(stats.ttest_ind(dff[dff['regression_type'] == 'hehe1']['reg'].to_numpy(), dff[dff['regression_type'] == 'baseline_regresion_score']['reg'].to_numpy()))\n",
    "    ### t-test hehe2 vs refined_regression_score\n",
    "    ##print(stats.ttest_ind(dff[dff['regression_type'] == 'hehe2']['reg'].to_numpy(), dff[dff['regression_type'] == 'refined_regression_score']['reg'].to_numpy()))\n",
    "    \n",
    "    # remove mixed_regression_score_scoring_rmse, mixed_regression_score_lerp_rmse\n",
    "    dff = dff[dff['regression_type'] != 'mixed_regression_score_scoring']\n",
    "    dff = dff[dff['regression_type'] != 'mixed_regression_score_lerp']\n",
    "\n",
    "    # print correlation of reg and mean_grade_bucket\n",
    "    print(\"CORR\", dff.groupby(['regression_type']).corr()['reg'])\n",
    "\n",
    "    sns.lineplot(x='mean_grade_bucket', y='reg', hue='regression_type', palette='flare', data=dff, style='regression_type', errorbar='sd', markers=True, dashes=False)\n",
    "    plt.title(\"regression result, entire dataset\")\n",
    "    plt.show()\n",
    "\n",
    "draw1()\n",
    "draw2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_to(n, fraction):\n",
    "    return round(n / fraction) * fraction\n",
    "\n",
    "\n",
    "loaded_df = pd.read_pickle(\"val_df_normalization_scores.pkl\")\n",
    "\n",
    "loaded_df['mean_grade_bucket'] = loaded_df['meanGrade'].apply(lambda x: round_to(x, 0.2))\n",
    "\n",
    "# calculate mean of baseline_regression_score when grouped by mean_grade_bucket\n",
    "# store it as a scalar value\n",
    "mean_baseline_regression_score = loaded_df.groupby(['mean_grade_bucket']).mean().reset_index()['baseline_regresion_score'].mean()\n",
    "mean_refined_regression_score  = loaded_df.groupby(['mean_grade_bucket']).mean().reset_index()['refined_regression_score'].mean()\n",
    "\n",
    "# same but stddev\n",
    "std_baseline_regression_score = loaded_df.groupby(['mean_grade_bucket']).mean().reset_index()['baseline_regresion_score'].std()\n",
    "std_refined_regression_score  = loaded_df.groupby(['mean_grade_bucket']).mean().reset_index()['refined_regression_score'].std()\n",
    "\n",
    "print(mean_baseline_regression_score, std_baseline_regression_score)\n",
    "print(mean_refined_regression_score, std_refined_regression_score)\n",
    "\n",
    "regression_cols = [\n",
    "    'baseline_regresion_score',\n",
    "    'refined_regression_score',\n",
    "    'mixed_regression_score_scoring',\n",
    "    'mixed_regression_score_lerp',\n",
    "]\n",
    "\n",
    "for col in regression_cols:\n",
    "    loaded_df[col + '_rmse'] = loaded_df.apply(lambda row: np.abs((row[col] - row[\"normalized_score\"])) , axis=1)\n",
    "\n",
    "# create column mean_grade_bucket, rounded to nearest 0.1\n",
    "\n",
    "\n",
    "print(loaded_df[['id', 'meanGrade', 'mean_grade_bucket']].head(10))\n",
    "# transform so that columns baseline_regresion_score, refined_regression_score, mixed_regression_score_scoring, mixed_regression_score_lerp\n",
    "# become values of a new column \"regression_type\" and their values become values of column rmse\n",
    "def draw1():\n",
    "    dff = loaded_df.melt(\n",
    "        id_vars=['id', 'meanGrade', 'mean_grade_bucket'], \n",
    "        value_vars=[\n",
    "            'baseline_regresion_score_rmse', \n",
    "            'refined_regression_score_rmse', \n",
    "            'mixed_regression_score_scoring_rmse', \n",
    "            'mixed_regression_score_lerp_rmse',\n",
    "        ],\n",
    "        var_name='regression_type', \n",
    "        value_name='rmse'\n",
    "    )\n",
    "\n",
    "    # remove mixed_regression_score_scoring_rmse, mixed_regression_score_lerp_rmse\n",
    "    dff = dff[dff['regression_type'] != 'mixed_regression_score_scoring_rmse']\n",
    "    dff = dff[dff['regression_type'] != 'mixed_regression_score_lerp_rmse']\n",
    "\n",
    "    \n",
    "    \n",
    "    sns.lineplot(x='mean_grade_bucket', y='rmse', hue='regression_type', palette='flare', data=dff, style='regression_type', errorbar='sd', markers=True, dashes=False)\n",
    "    plt.title(\"entire dataset\")\n",
    "    plt.show()\n",
    "\n",
    "def draw2():\n",
    "    dff = loaded_df.melt(\n",
    "        id_vars=['id', 'meanGrade', 'mean_grade_bucket'], \n",
    "        value_vars=regression_cols,\n",
    "        var_name='regression_type', \n",
    "        value_name='reg'\n",
    "    )\n",
    "    dff['reg'] *= 3\n",
    "\n",
    "    # remove mixed_regression_score_scoring_rmse, mixed_regression_score_lerp_rmse\n",
    "    dff = dff[dff['regression_type'] != 'mixed_regression_score_scoring']\n",
    "    dff = dff[dff['regression_type'] != 'mixed_regression_score_lerp']\n",
    "\n",
    "    sns.lineplot(x='mean_grade_bucket', y='reg', hue='regression_type', palette='flare', data=dff, style='regression_type', errorbar='sd', markers=True, dashes=False)\n",
    "    plt.title(\"regression result, entire dataset\")\n",
    "    plt.show()\n",
    "\n",
    "draw1()\n",
    "draw2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import *\n",
    "def test():\n",
    "    train_df, val_df, test_df = make_base_dataset()\n",
    "    train_df_please = train_df[(train_df['mean_grade_bucket'] != 1.0)]\n",
    "    val_df_please   = val_df[(val_df['mean_grade_bucket'] != 1.0)]\n",
    "    test_df_please  = test_df[(test_df['mean_grade_bucket'] != 1.0)]\n",
    "    print(len(train_df), len(val_df), len(test_df))\n",
    "    print(len(train_df_please), len(val_df_please), len(test_df_please))\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "from dieboldmariano import dm_test\n",
    "\n",
    "def round_to(n, fraction):\n",
    "    return round(n / fraction) * fraction\n",
    "\n",
    "\n",
    "def load_and_plot_df(ax1, ax2, path, label, mae_results, mse_results, reg_results):\n",
    "    loaded_df = pd.read_pickle(path)\n",
    "\n",
    "    loaded_df['mean_grade_bucket'] = loaded_df['meanGrade'].apply(lambda x: round_to(x, 0.5))\n",
    "\n",
    "    loaded_df['baseline_regression_score_mse'] = loaded_df.apply(lambda row: ((row['baseline_regression_score']*3 - row[\"meanGrade\"])**2) , axis=1)\n",
    "    loaded_df['cleaned_regression_score_mse']  = loaded_df.apply(lambda row: ((row['cleaned_regression_score'] *3 - row[\"meanGrade\"])**2) , axis=1)\n",
    "\n",
    "    # sum all rmses for each mean grade bucket, and divide by number of buckets\n",
    "    average_rmse_baseline = np.sqrt(loaded_df.groupby(['mean_grade_bucket']).mean().reset_index()['baseline_regression_score_mse'].mean())\n",
    "    average_rmse_cleaned  = np.sqrt(loaded_df.groupby(['mean_grade_bucket']).mean().reset_index()['cleaned_regression_score_mse'].mean())\n",
    "    \n",
    "    print(f\"[{label}] total RMSE, baseline:\", average_rmse_baseline)\n",
    "    print(f\"[{label}] total RMSE, cleaned: \", average_rmse_cleaned)\n",
    "    \n",
    "    def draw1():\n",
    "        dff = loaded_df.melt(\n",
    "            id_vars=['id', 'meanGrade', 'mean_grade_bucket'], \n",
    "            value_vars=[\n",
    "                'baseline_regression_score_mse', \n",
    "                'cleaned_regression_score_mse', \n",
    "            ],\n",
    "            var_name='regression_type', \n",
    "            value_name='mse'\n",
    "        )\n",
    "\n",
    "        dff['mae'] = dff.apply(lambda row: np.sqrt(row['mse']), axis=1)\n",
    "\n",
    "        # make dataframe that contains the mse\n",
    "        dff_mse = dff.groupby(['mean_grade_bucket', 'regression_type']).mean().reset_index()\n",
    "        # apply sqrt\n",
    "        dff_mse['rmse'] = dff_mse.apply(lambda row: np.sqrt(row['mse']), axis=1)\n",
    "\n",
    "        # compute pearson correlation coefficient\n",
    "        # print(\"PEARSON\", dff.groupby(['regression_type']).corr()['rmse'])\n",
    "\n",
    "        tests = [\n",
    "            (dff[dff['meanGrade'] == dff['meanGrade'].quantile(0.05)], \"bottom 5%\"),\n",
    "            (dff[dff['meanGrade'] == dff['meanGrade'].quantile(0.1)], \"bottom 10%\"),\n",
    "            (dff[dff['meanGrade'] <= 0.4], \"below Q1\"),\n",
    "            (dff[(dff['meanGrade'] >= 0.4) & (dff['meanGrade'] <= 1.4)], \"between Q1 and Q3\"),\n",
    "            (dff[dff['meanGrade'] >= 1.4], \"above Q3\"),\n",
    "            (dff, \"entire dataset\")\n",
    "        ]\n",
    "        for dfx, segment_name in tests:\n",
    "            dfx1 = dfx[dfx['regression_type'] == 'baseline_regression_score_mse']\n",
    "            dfx2 = dfx[dfx['regression_type'] == 'cleaned_regression_score_mse']\n",
    "\n",
    "            pval = stats.ttest_ind(dfx1['mae'].to_numpy(), dfx2['mae'].to_numpy()).pvalue\n",
    "            print(f\"[{label}] t test for mae, {segment_name}\", pval)\n",
    "\n",
    "            mae_results.append({\n",
    "                \"model\": label,\n",
    "                \"segment\": segment_name,\n",
    "                \"pval\": pval,\n",
    "                \"significant\": pval < 0.05,\n",
    "                \"mae_baseline\": dfx1['mae'].mean(),\n",
    "                \"mae_cleaned\":  dfx2['mae'].mean(),\n",
    "            })\n",
    "\n",
    "            pval = stats.ttest_ind(dfx1['mse'].to_numpy(), dfx2['mse'].to_numpy()).pvalue\n",
    "            print(f\"[{label}] t test for mse, {segment_name}\", pval)            \n",
    "            mse_results.append({\n",
    "                \"model\": label,\n",
    "                \"segment\": segment_name,\n",
    "                \"pval\": pval,\n",
    "                \"significant\": pval < 0.05,\n",
    "                \"mse_baseline\": dfx1['mse'].mean(),\n",
    "                \"mse_cleaned\":  dfx2['mse'].mean(),\n",
    "            })\n",
    "\n",
    "        sns.lineplot(x='mean_grade_bucket', y='mae', hue='regression_type', palette='flare', data=dff, style='regression_type', errorbar=None, markers=True, dashes=False, ax=ax1)\n",
    "        \n",
    "        # make dataframe that contains the mse\n",
    "        dff_mse = dff.groupby(['mean_grade_bucket', 'regression_type']).mean().reset_index()\n",
    "        # apply sqrt\n",
    "        dff_mse['rmse'] = dff_mse.apply(lambda row: np.sqrt(row['mse']), axis=1)\n",
    "\n",
    "        display_copy = dff_mse.copy().rename(columns={\n",
    "            'baseline_regression_score_mse': 'Baseline',\n",
    "            'cleaned_regression_score_mse': 'NoMiddle50',\n",
    "            'mean_grade_bucket': 'Mean Grade',\n",
    "            'mse': 'MSE',\n",
    "            'rmse': 'RMSE',\n",
    "            'regression_type': 'Model',\n",
    "        })\n",
    "        display_copy['Model'].replace('baseline_regression_score_mse', 'Baseline', inplace=True)\n",
    "        display_copy['Model'].replace('cleaned_regression_score_mse', 'NoMiddle50', inplace=True)\n",
    "\n",
    "        sns.lineplot(x='Mean Grade', y='RMSE', hue='Model', palette='flare', data=display_copy, style='Model', errorbar=None, markers=True, dashes=False)\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "    def draw2():\n",
    "        dff = loaded_df.melt(\n",
    "            id_vars=['id', 'meanGrade', 'mean_grade_bucket'], \n",
    "            value_vars=[\n",
    "                'baseline_regression_score', \n",
    "                'cleaned_regression_score', \n",
    "            ],\n",
    "            var_name='regression_type', \n",
    "            value_name='reg'\n",
    "        )\n",
    "        dff['reg'] *= 3\n",
    "\n",
    "\n",
    "        # print correlation of reg and mean_grade_bucket\n",
    "        # print(\"CORR\", dff.groupby(['regression_type']).corr()['reg'])\n",
    "\n",
    "        tests = [\n",
    "            (dff[dff['meanGrade'] <= 0.4], \"below Q1\"),\n",
    "            (dff[(dff['meanGrade'] >= 0.4) & (dff['meanGrade'] <= 1.4)], \"between Q1 and Q3\"),\n",
    "            (dff[dff['meanGrade'] >= 1.4], \"above Q3\"),\n",
    "        ]\n",
    "        for dfx, segment_name in tests:\n",
    "            dfx1 = dfx[dfx['regression_type'] == 'baseline_regression_score']\n",
    "            dfx2 = dfx[dfx['regression_type'] == 'cleaned_regression_score']\n",
    "            pval = stats.ttest_ind(dfx1['reg'].to_numpy(), dfx2['reg'].to_numpy()).pvalue\n",
    "            print(f\"[{label}] t test for regression, {segment_name}\", pval)\n",
    "            reg_results.append({\n",
    "                \"model\": label,\n",
    "                \"segment\": segment_name,\n",
    "                \"pval\": pval,\n",
    "                \"significant\": pval < 0.05,\n",
    "                \"reg_baseline\": dfx1['reg'].mean(),\n",
    "                \"reg_cleaned\":  dfx2['reg'].mean(),\n",
    "            })\n",
    "\n",
    "        sns.lineplot(x='mean_grade_bucket', y='reg', hue='regression_type', palette='flare', data=dff, style='regression_type', errorbar=None, markers=True, dashes=False, ax=ax2)\n",
    "        \n",
    "    draw1()\n",
    "    draw2()\n",
    "\n",
    "fig1, axes1 = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n",
    "fig2, axes2 = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n",
    "\n",
    "mae_results = []\n",
    "mse_results = []\n",
    "reg_results = []\n",
    "load_and_plot_df(axes1[0], axes2[0], \"test_df_with_scores.pkl\",      \"< Q1 removed\", mae_results, mse_results, reg_results)\n",
    "load_and_plot_df(axes1[1], axes2[1], \"test_df_with_scores_2pls.pkl\", \"Q1 - Q3 removed\", mae_results, mse_results, reg_results)\n",
    "load_and_plot_df(axes1[2], axes2[2], \"test_df_with_scores3.pkl\",     \"> Q3 removed\", mae_results, mse_results, reg_results)\n",
    "\n",
    "for ax in axes1:\n",
    "    ax.set_xlim([0-0.1, 3+0.1])\n",
    "    ax.set_ylim([0, 2+0.1])\n",
    "for ax in axes2:\n",
    "    ax.set_xlim([0, 3])\n",
    "    ax.set_ylim([0, 3])\n",
    "\n",
    "fig1.show()\n",
    "fig2.show()\n",
    "\n",
    "pd.DataFrame(mse_results).to_excel(\"mse_results.xlsx\")\n",
    "pd.DataFrame(mae_results).to_excel(\"mae_results.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of rows where most_common_grade == 0 and atleast one grade is 3\n",
    "\n",
    "from collections import defaultdict\n",
    "counts = defaultdict(int)\n",
    "for _, row in combined_df.iterrows():\n",
    "    has_1 = np.sum(row['grades'] >= 1) > 1\n",
    "    has_2 = np.sum(row['grades'] >= 2) > 1\n",
    "    has_3 = np.sum(row['grades'] >= 3) > 1\n",
    "\n",
    "    if has_1 and row['most_common_grade'] != 1:\n",
    "        counts[(row['most_common_grade'], 1)] += 1\n",
    "    if has_2 and row['most_common_grade'] != 2:\n",
    "        counts[(row['most_common_grade'], 2)] += 1\n",
    "    if has_3 and row['most_common_grade'] != 3:\n",
    "        counts[(row['most_common_grade'], 3)] += 1\n",
    "\n",
    "for most_common, grade in sorted(counts):\n",
    "    print(most_common, grade, counts[(most_common, grade)])\n",
    "\n",
    "print(len(combined_df[combined_df['most_common_grade'] == 0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.array([1, 2, 3, 4]) >= 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(combined_df[combined_df['meanGrade'] <= 0.55]) / len(combined_df) + \\\n",
    "len(combined_df[combined_df['meanGrade'] >= 1.8]) / len(combined_df)\n",
    "\n",
    "# calculate quantiles\n",
    "q1 = combined_df['meanGrade'].quantile(0.25)\n",
    "q3 = combined_df['meanGrade'].quantile(0.75)\n",
    "print(q1, q3)\n",
    "\n",
    "# calculate mean grade in combined_df\n",
    "m = combined_df['meanGrade'].mean()\n",
    "# calculate stddev of meanGrade\n",
    "s = combined_df['meanGrade'].std()\n",
    "\n",
    "left  = combined_df[combined_df['meanGrade'] <= m -s]\n",
    "right = combined_df[combined_df['meanGrade'] >= m + s]\n",
    "print(len(left) / len(combined_df) + len(right) / len(combined_df))\n",
    "\n",
    "print(m-s, m+s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(combined_df[combined_df['meanGrade'] >= 2.0]) / len(combined_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grade_groups(dataset,colname='grades_max_5'):\n",
    "    gradecount=dict()\n",
    "    for e in dataset[colname]:\n",
    "        f=tuple(e)\n",
    "        n=gradecount.get(f,0)\n",
    "        gradecount[f]=n+1\n",
    "    return gradecount\n",
    "def counter_dict(iL,D=dict(),ro=-1):\n",
    "    print(ro,len(D))\n",
    "    for el in iL:\n",
    "        if ro<0:\n",
    "            elm=el\n",
    "        else:\n",
    "            elm = round(el,ro)\n",
    "            if len(str(elm))>7:\n",
    "                print(el,elm)\n",
    "        val=D.get(elm,0)\n",
    "        D[elm]=val+1\n",
    "    return D\n",
    "\n",
    "def get_common_words(dataset):\n",
    "    gradecount=dict()\n",
    "    for index,row in df.iterrows():\n",
    "        E=row['edited_sentence'].split()\n",
    "        F=row['original'].split()\n",
    "        E+=[e[1:][:-2] for e in F if e not in E]\n",
    "        counter_dict(E,gradecount)\n",
    "    return gradecount\n",
    "def descending_common(D,n=0):\n",
    "    X=[(v,e) for (e,v) in D.items()]\n",
    "    X.sort(reverse=True)\n",
    "    return X if n==0 else X[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words_dict=get_common_words(combined_df)\n",
    "descending_common(common_words_dict,5),[common_words_dict.get(e) for e in [\"Biden\",\"Trump\",\"Obama\",\"Bush\",\"Clinton\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg(X):\n",
    "    if len(X)==0:\n",
    "        return 0\n",
    "    return sum(X)/len(X)\n",
    "def group_annotator_scores(occurence_dict:dict,grouping=[0,0,1,2,2],resolver=avg):\n",
    "    n=max(grouping)+1\n",
    "    newdict=dict()\n",
    "    for key,amount in occurence_dict.items():\n",
    "        values=[list() for i in range(n)]\n",
    "        for i in range(len(key)):\n",
    "            if key[i]<0:\n",
    "                continue\n",
    "            values[grouping[i]].append(key[i])\n",
    "        newkey=tuple([resolver(E)for E in values])\n",
    "        current=newdict.get(newkey,0)\n",
    "        newdict[newkey]=current+amount\n",
    "    return newdict\n",
    "group_annotator_scores({(1,10,100,1000,10000):43})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_groups=get_grade_groups(df)\n",
    "print(len(grade_groups))\n",
    "print(grade_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_groups_3=group_annotator_scores(grade_groups,resolver=avg)\n",
    "print(len(grade_groups_3))\n",
    "print(grade_groups_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_groups_1=group_annotator_scores(grade_groups,[0,0,0,0,0],resolver=avg)\n",
    "print(len(grade_groups_1))\n",
    "print(grade_groups_1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to make a custom grade generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateCustomGrader(weights=[1,2,3,4,5],top_grade=3):\n",
    "    saved_weights=weights[:]\n",
    "    bottom_weights=[min(e,0) for e in weights]\n",
    "    top_weights=[max(e,0) for e in weights]\n",
    "    minval=sum(bottom_weights)\n",
    "    maxval=sum(top_weights)\n",
    "    if minval==maxval:\n",
    "        return lambda X:0\n",
    "    a=top_grade/(maxval-minval)\n",
    "    b=0-a*minval\n",
    "    xm=0-minval*top_grade\n",
    "    print(minval, maxval)\n",
    "    def CustomGrader(X):\n",
    "        if len(X)!=len(saved_weights):\n",
    "            raise Exception(\"Bad weights size! {} vs {}\".format(X,saved_weights))\n",
    "        x=xm\n",
    "        for i in range(len(X)):\n",
    "            x+=X[i]*saved_weights[i]\n",
    "        x*=a\n",
    "        return x\n",
    "    return CustomGrader\n",
    "def ApplyCustomGrader(df,grader,oldname,newname):\n",
    "    return pd.DataFrame({newname: df[oldname].apply(grader)})\n",
    "CG_test=GenerateCustomGrader([-2,-1,0,1,2])\n",
    "for E in [(0,0,0,0,0),(3,3,3,3,3),(0,0,0,3,3),(3,3,0,0,0)]:\n",
    "    print(E,CG_test(E))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DisplayGradeset():\n",
    "    sns.set_style(\"ticks\")\n",
    "    sns.histplot(combined_df['meanGrade'], palette=\"flare\", bins=16, cumulative=False)\n",
    "    plt.xlabel('Mean Grade')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n",
    "\n",
    "    len(combined_df[combined_df['meanGrade'] <= 0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_grader=GenerateCustomGrader()\n",
    "print(combined_df.grades_max_5)\n",
    "# temp=ApplyCustomGrader(combined_df,example_grader,'grades','grades')\n",
    "# print(temp.columns)\n",
    "# len(get_grade_groups(combined_df,'grades'))\n",
    "temp=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_uneven_bars(grades_dict):\n",
    "    L=[(e,v) for (e,v) in grades_dict.items()]\n",
    "    L.sort()\n",
    "    limits=[]\n",
    "    last=L[0][0]\n",
    "    for (e,_) in L:\n",
    "        limits.append((last+e)/2)\n",
    "        last=e\n",
    "    limits.append(L[-1][0])\n",
    "    widths=[limits[i]-limits[i-1] for i in range(1,len(limits))]\n",
    "    data=[v for (_,v) in L]\n",
    "    print(limits)\n",
    "    print(widths)\n",
    "    print(data)\n",
    "    plt.bar(limits[:-1], data, width=widths, align='edge')\n",
    "\n",
    "    # Set labels and title\n",
    "    plt.xlabel('Bins')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Histogram with Varying Bar Widths')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "display_uneven_bars({0:1,0.5:2,1.5:3,2:4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=counter_dict(combined_df['meanGrade'],dict())\n",
    "print(X)\n",
    "Y=counter_dict(combined_df['meanGrade'],dict(),ro=3)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Environment",
   "language": "python",
   "name": "tpenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
